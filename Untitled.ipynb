{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le jeu de données est déjà téléchargé et extrait!\n",
      "bedroom = 0\n",
      "CALsuburb = 1\n",
      "kitchen = 2\n",
      "livingroom = 3\n",
      "MITcoast = 4\n",
      "MITforest = 5\n",
      "MIThighway = 6\n",
      "MITinsidecity = 7\n",
      "MITmountain = 8\n",
      "MITopencountry = 9\n",
      "MITstreet = 10\n",
      "MITtallbuilding = 11\n",
      "PARoffice = 12\n",
      "total categories: 13\n",
      "Taille totale du jeu de données: 3859\n",
      "Total des fichiers du jeu de données : 3859\n",
      "Total des labels dans le jeu de données: 3859\n",
      "Nombre total des fichiers du split entrainement: 1300\n",
      "Nombre total des fichiers du split de test: 2559\n",
      "...Chargement du fichier de kmean...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/1300 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'extract_daisy_feature_from_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e9abd83a9275>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_filenames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mXTRAIN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_daisy_feature_from_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_filenames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdaisy_cluster_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m         \u001b[0mYTRAIN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[0mclassifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m1.6794140624999994\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m0.1630955304365928\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecision_function_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ovo'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#paramètres de la fonction noyau SVM\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'extract_daisy_feature_from_image' is not defined"
     ]
    }
   ],
   "source": [
    "# %load scenereclassificationTest.py\n",
    "\n",
    "#%load scenereclassificationTest.py\n",
    "from os.path import isfile, join, exists\n",
    "import os.path\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlretrieve\n",
    "from os import listdir\n",
    "from sklearn.externals import joblib\n",
    "#Pour la division de l'ensemble de données dans un ensemble de formation (100 instances) et un ensemble de test (le reste) #\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Routines de traitement d'image pour extraction / transformation de fonctionnalités\n",
    "from skimage.feature import daisy,hog#,sift\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "import skimage\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Télécharger le jeu de données\n",
    "if not exists('SceneClass13/'):\n",
    "    if not exists('SceneClass13.rar'):\n",
    "        print ('Téléchargement de SceneClass13.rar')\n",
    "        urlretrieve ('http://vision.stanford.edu/Datasets/SceneClass13.rar')\n",
    "        print ('Téléchargement de SceneClass13.rar')\n",
    "    print ('Extraction de SceneClass13.rar')\n",
    "    zipfile = ZipFile('SceneClass13.rar', 'r')\n",
    "    zipfile.extractall('./SceneClass13')\n",
    "    zipfile.close()\n",
    "    print ('Déjà extraite SceneClass13.rar')\n",
    "else:\n",
    "    print ('Le jeu de données est déjà téléchargé et extrait!')\n",
    "\n",
    "\n",
    "#Obtenez tous les noms de fichiers (y compris le chemin complet) dans un dossier en tant que liste.\n",
    "def get_filenames(path):\n",
    "    onlyfiles = [path+f for f in listdir(path) if (isfile(join(path, f)) and (f.find(\"Thumbs.db\")==-1))]\n",
    "    return onlyfiles\n",
    "# Descripteur local SIFT \n",
    "#Fonction pour extraire les fonctionnalités de SIFT ainsi que les fonctions de HOG à partir d'une image\n",
    "#def extract_daisy_and_hog_features_from_image(file_path,daisy_step_size=32,daisy_radius=32,hog_pixels_per_cell=16,hog_cells_per_block=1):\n",
    "#    img = io.imread(file_path)\n",
    "#    img_gray = rgb2gray(img)\n",
    "#    img=skimage.transform.resize(img_gray,(300,250)) ##resize to a suitable dimension, avg size of images in the dataset\n",
    "    #original, histograms=6\n",
    "#    descs = sift(img, step=daisy_step_size, radius=daisy_radius, rings=2, histograms=8,orientations=8, visualize=False)\n",
    "    #Calculer les descripteurs de caractéristiques de marguerite\n",
    "#    descs_num = descs.shape[0] * descs.shape[1]\n",
    "#    sift_desriptors=descs.reshape(descs_num,descs.shape[2])\n",
    "#  \n",
    "#    return sift_desriptors,hog_desriptor\n",
    "\n",
    "#Fonction pour extraire les fonctionnalités de HOG\n",
    "def extract_hog_from_image(file_path,daisy_step_size=32,daisy_radius=32,hog_pixels_per_cell=16,hog_cells_per_block=1):\n",
    "    img = io.imread(file_path)\n",
    "    img_gray = rgb2gray(img)\n",
    "    img=skimage.transform.resize(img_gray,(300,250)) ##resize to a suitable dimension, avg size of images in the dataset\n",
    "    #original, histograms=6\n",
    "    descs = daisy(img, step=daisy_step_size, radius=daisy_radius, rings=2, histograms=8,orientations=8, visualize=False)\n",
    "    #Calculer les descripteurs de caractéristiques de marguerite\n",
    "    descs_num = descs.shape[0] * descs.shape[1]\n",
    "    #daisy_desriptors=descs.reshape(descs_num,descs.shape[2])\n",
    "    hog_desriptor=hog(img, orientations=8, pixels_per_cell=(hog_pixels_per_cell, hog_pixels_per_cell),cells_per_block=(hog_cells_per_block, hog_cells_per_block), visualise=False,feature_vector=True)\n",
    "    return hog_desriptor\n",
    "\n",
    "\n",
    "base_path=\"SceneClass13/\" #Chemin où les données d'image sont conservées\n",
    "img_width=300\n",
    "img_height=250\n",
    "hog_pixels_per_cell=16\n",
    "orientations=8\n",
    "#Chargez les noms de fichiers correspondant à chaque catégorie de scène en listes\n",
    "category_names=listdir(base_path) ##\n",
    "for i in range(len(category_names)):\n",
    "    print (category_names[i],'=',i)\n",
    "print ('total categories:',len(category_names))\n",
    "dataset_filenames=[] #Liste pour garder le chemin de tous les fichiers dans la base de données\n",
    "dataset_labels=[]\n",
    "##category_names[0] Liste la représentation textuelle de l'identifiant de catégorie\n",
    "for category in category_names:\n",
    "    category_filenames=get_filenames(base_path+category+\"/\")##get all the filenames in that category\n",
    "    category_labels=np.ones(len(category_filenames))*category_names.index(category) ##label the category with its index position\n",
    "    dataset_filenames=dataset_filenames+category_filenames\n",
    "    dataset_labels=dataset_labels+list(category_labels)\n",
    "    #for num in range(0,100):\n",
    "    #    train_filenames = train_filenames + \n",
    "print ('Taille totale du jeu de données:',len(dataset_filenames))\n",
    "#Split into training files and testing files\n",
    "#sss = StratifiedShuffleSplit(dataset_labels,train_size=100, random_state=0)\n",
    "#train_index, test_index = [s for s in sss.split(dataset_filenames,dataset_labels)][0]\n",
    "#train_filenames, test_filenames, train_labels, test_labels = dataset_filenames[train_index, :], dataset_labels[train_index], dataset_filenames[test_index, :], dataset_labels[test_index]\n",
    "#for train_index, test_index in sss:\n",
    "#    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#    train_filenames, test_filenames = dataset_filenames[train_index], dataset_filenames[test_index]\n",
    "#    train_labels, test_labels = dataset_labels[train_index], dataset_labels[test_index]\n",
    "print ('Total des fichiers du jeu de données :',len(dataset_filenames))\n",
    "print ('Total des labels dans le jeu de données:',len(dataset_labels))\n",
    "train_filenames, test_filenames, train_labels, test_labels = train_test_split(dataset_filenames,dataset_labels,train_size=1300, stratify=dataset_labels)\n",
    "#train_filenames,test_filenames,train_labels,test_labels=train_test_split(dataset_filenames,dataset_labels,train_size =100,stratify=dataset_labels)\n",
    "print ('Nombre total des fichiers du split entrainement:',len(train_filenames))\n",
    "print ('Nombre total des fichiers du split de test:',len(test_filenames))\n",
    "#Extraire les fonctionnalités de la division de données de formation pour le traitement en aval, prend environ 12 minutes pour un ordinateur portable standard\n",
    "training_data_feature_map={} #map pour stocker les caractéristiques DAISY ainsi que la caractéristique HOG pour tous les points de données de entrainement\n",
    "daisy_descriptor_list=[] #Liste pour stocker tous les descripteurs DAISY pour former notre vocabulaire visuel en regroupant\n",
    "counter=0\n",
    "\n",
    "#Maintenant, pour former des \"mots visuels\", nous agrandissons les caractéristiques de DAISY pour former un vocabulaire, nous formons une caractéristique d'histogramme (histogramme de DAISY) correspondant à chaque caractéristique des dimensions 'number_of_clusters'\n",
    "\n",
    "#Entrée: liste des caractéristiques DAISY  et nombre de clusters\n",
    "#Sorite: a trained cluster model which will allow to get the cluster id corresponding to any input daisy feature\n",
    "def cluster_daisy_features(daisy_feature_list,number_of_clusters):\n",
    "    #km=KMeans(n_clusters=number_of_clusters)\n",
    "    km=MiniBatchKMeans(n_clusters=number_of_clusters,batch_size=number_of_clusters*10)\n",
    "    km.fit(daisy_feature_list)\n",
    "    return km\n",
    "# cacher les  warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#Le nombre de clusters est défini comme 600 #, prend plusieurs minutes pour fonctionner sur un ordinateur  standard\n",
    "filename1 = 'kmean.pkl'\n",
    "\n",
    "if os.path.exists(filename1):\n",
    "   # load the model from disk\n",
    "    print ('...Chargement du fichier de kmean...')\n",
    "    daisy_cluster_model = joblib.load(filename1)\n",
    "else:\n",
    "    for fname in tqdm(train_filenames):\n",
    "        daisy_features,hog_feature=extract_daisy_and_hog_features_from_image(fname,daisy_step_size=8,daisy_radius=8)\n",
    "    #Extraire les fonctionnalités DAISY et les fonctions HOG de l'image et enregistrer dans une map\n",
    "        training_data_feature_map[fname]=[daisy_features,hog_feature]\n",
    "        daisy_descriptor_list=daisy_descriptor_list+list(daisy_features)\n",
    "    print ('Total daisy descriptors:',len(daisy_descriptor_list))\n",
    "    print ('...Enregistrement du modèle...')\n",
    "    daisy_cluster_model=cluster_daisy_features(daisy_descriptor_list,600) \n",
    "    daisy_cluster_model.n_clusters\n",
    "    joblib.dump(daisy_cluster_model, filename1)\n",
    "\n",
    "#Fonction pour extraire la fonction hybride des images en regroupant l'histogramme de DAISY et le descripteur de HOG après l'individu\n",
    "def extract_hog_feature_from_image(fname,daisy_cluster_model):\n",
    "    #Dans le cas où nous aurions rencontré le fichier lors de l'entrainement, les caractéristiques DAISY et HOG auraient déjà été calculées\n",
    "    if fname in training_data_feature_map:\n",
    "        daisy_features=training_data_feature_map[fname][0]\n",
    "        hog_feature=training_data_feature_map[fname][1]\n",
    "    else:\n",
    "        daisy_features=extract_daisy_from_image(fname,daisy_step_size=8,daisy_radius=8)\n",
    "        \n",
    "    # Indiquer à quoi appartiennent les cluster de chaque caractéristique DAISY\n",
    "    img_clusters=daisy_cluster_model.predict(daisy_features) \n",
    "    cluster_freq_counts=pd.DataFrame(img_clusters,columns=['cnt'])['cnt'].value_counts()\n",
    "    bovw_vector=np.zeros(daisy_cluster_model.n_clusters) ##feature vector of size as the total number of clusters\n",
    "    for key in cluster_freq_counts.keys():\n",
    "        bovw_vector[key]=cluster_freq_counts[key]\n",
    "\n",
    "    #bovw_feature=bovw_vector/np.linalg.norm(bovw_vector)\n",
    "    hog_feature=hog_feature/np.linalg.norm(hog_feature)\n",
    "    return list(hog_feature)\n",
    "\n",
    "def plot_file(fname):\n",
    "    img_data=plt.imread(fname)\n",
    "    plt.imshow(rgb2gray(img_data),cmap='Greys_r')\n",
    "#Extraction de caractéristiques de données d'entrainement\n",
    "XTRAIN=[]\n",
    "YTRAIN=[]\n",
    "filename = 'trainedsvm.pkl'\n",
    "\n",
    "if os.path.exists(filename):\n",
    "   # load the model from disk\n",
    "    print ('...Chargement du fichier apprentissage...')\n",
    "    classifier = joblib.load(filename)\n",
    "else:\n",
    "    for i in tqdm(range(len(train_filenames))):\n",
    "        XTRAIN.append(extract_daisy_feature_from_image(train_filenames[i],daisy_cluster_model))\n",
    "        YTRAIN.append(train_labels[i])\n",
    "    classifier=svm.SVC(C=10**1.6794140624999994, gamma=10**-0.1630955304365928, decision_function_shape='ovo') #paramètres de la fonction noyau SVM\n",
    "    print ('...Enregistrement du modèle...')\n",
    "    classifier.fit(XTRAIN,YTRAIN)\n",
    "    print ('...Enregistrement du fichier apprentissage...')\n",
    "    joblib.dump(classifier, filename)\n",
    "\n",
    "# Montrer un example de classification\n",
    "plot_file(test_filenames[4])\n",
    "print ('true label:',test_labels[4])\n",
    "feature_vector=extract_daisy_feature_from_image(test_filenames[4],daisy_cluster_model)\n",
    "print ('prediction:',classifier.predict([feature_vector]))\n",
    "\n",
    "## Test de l'extraction des fonctions de données, ne faites que pour quelques instants si un test rapide est nécessaire\n",
    "XTEST=[]\n",
    "YTEST=[]\n",
    "for i in tqdm(range(len(test_filenames))):\n",
    "    XTEST.append(extract_hog_feature_from_image(test_filenames[i],daisy_cluster_model))\n",
    "    YTEST.append(test_labels[i])\n",
    "print ('Métriques du classifieur hybride')\n",
    "print ('Nombre instances test:',len(XTEST),len(YTEST))\n",
    "#Rapport de précision\n",
    "pred=classifier.predict(XTEST)\n",
    "       \n",
    "print ('Précision globale: \\n',accuracy_score(YTEST,pred))\n",
    "print (classification_report(YTEST, pred, target_names=category_names))\n",
    "print ('Matrice de confusion:\\n')\n",
    "pd.DataFrame(confusion_matrix(YTEST,pred),\n",
    "            columns=['MITinsidecity', 'bedroom', 'PARoffice', 'MITmountain', 'MITtallbuilding', 'MIThighway', 'MITcoast', 'livingroom', 'MITopencountry', 'MITstreet', 'MITforest', 'kitchen', 'CALsuburb'],\n",
    "               index=['MITinsidecity', 'bedroom', 'PARoffice', 'MITmountain', 'MITtallbuilding', 'MIThighway', 'MITcoast', 'livingroom', 'MITopencountry', 'MITstreet', 'MITforest', 'kitchen', 'CALsuburb']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le jeu de données est déjà téléchargé et extrait!\n",
      "bedroom = 0\n",
      "CALsuburb = 1\n",
      "kitchen = 2\n",
      "livingroom = 3\n",
      "MITcoast = 4\n",
      "MITforest = 5\n",
      "MIThighway = 6\n",
      "MITinsidecity = 7\n",
      "MITmountain = 8\n",
      "MITopencountry = 9\n",
      "MITstreet = 10\n",
      "MITtallbuilding = 11\n",
      "PARoffice = 12\n",
      "total categories: 13\n",
      "Taille totale du jeu de données: 3859\n",
      "Total des fichiers du jeu de données : 3859\n",
      "Total des labels dans le jeu de données: 3859\n",
      "Nombre total des fichiers du split entrainement: 1300\n",
      "Nombre total des fichiers du split de test: 2559\n",
      "...Chargement du fichier de kmean...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/1300 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'extract_daisy_from_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-91ae875551bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_filenames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mXTRAIN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_hog_feature_from_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_filenames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdaisy_cluster_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m         \u001b[0mYTRAIN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[0mclassifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m1.6794140624999994\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m0.1630955304365928\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecision_function_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ovo'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#paramètres de la fonction noyau SVM\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-91ae875551bb>\u001b[0m in \u001b[0;36mextract_hog_feature_from_image\u001b[1;34m(fname, daisy_cluster_model)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[0mhog_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_data_feature_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m         \u001b[0mdaisy_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextract_daisy_from_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdaisy_step_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdaisy_radius\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[1;31m# Indiquer à quoi appartiennent les cluster de chaque caractéristique DAISY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'extract_daisy_from_image' is not defined"
     ]
    }
   ],
   "source": [
    "# %load scenereclassificationTest.py\n",
    "\n",
    "#%load scenereclassificationTest.py\n",
    "from os.path import isfile, join, exists\n",
    "import os.path\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlretrieve\n",
    "from os import listdir\n",
    "from sklearn.externals import joblib\n",
    "#Pour la division de l'ensemble de données dans un ensemble de formation (100 instances) et un ensemble de test (le reste) #\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Routines de traitement d'image pour extraction / transformation de fonctionnalités\n",
    "from skimage.feature import daisy,hog#,sift\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "import skimage\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Télécharger le jeu de données\n",
    "if not exists('SceneClass13/'):\n",
    "    if not exists('SceneClass13.rar'):\n",
    "        print ('Téléchargement de SceneClass13.rar')\n",
    "        urlretrieve ('http://vision.stanford.edu/Datasets/SceneClass13.rar')\n",
    "        print ('Téléchargement de SceneClass13.rar')\n",
    "    print ('Extraction de SceneClass13.rar')\n",
    "    zipfile = ZipFile('SceneClass13.rar', 'r')\n",
    "    zipfile.extractall('./SceneClass13')\n",
    "    zipfile.close()\n",
    "    print ('Déjà extraite SceneClass13.rar')\n",
    "else:\n",
    "    print ('Le jeu de données est déjà téléchargé et extrait!')\n",
    "\n",
    "\n",
    "#Obtenez tous les noms de fichiers (y compris le chemin complet) dans un dossier en tant que liste.\n",
    "def get_filenames(path):\n",
    "    onlyfiles = [path+f for f in listdir(path) if (isfile(join(path, f)) and (f.find(\"Thumbs.db\")==-1))]\n",
    "    return onlyfiles\n",
    "# Descripteur local SIFT \n",
    "#Fonction pour extraire les fonctionnalités de SIFT ainsi que les fonctions de HOG à partir d'une image\n",
    "#def extract_daisy_and_hog_features_from_image(file_path,daisy_step_size=32,daisy_radius=32,hog_pixels_per_cell=16,hog_cells_per_block=1):\n",
    "#    img = io.imread(file_path)\n",
    "#    img_gray = rgb2gray(img)\n",
    "#    img=skimage.transform.resize(img_gray,(300,250)) ##resize to a suitable dimension, avg size of images in the dataset\n",
    "    #original, histograms=6\n",
    "#    descs = sift(img, step=daisy_step_size, radius=daisy_radius, rings=2, histograms=8,orientations=8, visualize=False)\n",
    "    #Calculer les descripteurs de caractéristiques de marguerite\n",
    "#    descs_num = descs.shape[0] * descs.shape[1]\n",
    "#    sift_desriptors=descs.reshape(descs_num,descs.shape[2])\n",
    "#  \n",
    "#    return sift_desriptors,hog_desriptor\n",
    "\n",
    "#Fonction pour extraire les fonctionnalités de HOG\n",
    "def extract_hog_from_image(file_path,daisy_step_size=32,daisy_radius=32,hog_pixels_per_cell=16,hog_cells_per_block=1):\n",
    "    img = io.imread(file_path)\n",
    "    img_gray = rgb2gray(img)\n",
    "    img=skimage.transform.resize(img_gray,(300,250)) ##resize to a suitable dimension, avg size of images in the dataset\n",
    "    #original, histograms=6\n",
    "    descs = daisy(img, step=daisy_step_size, radius=daisy_radius, rings=2, histograms=8,orientations=8, visualize=False)\n",
    "    #Calculer les descripteurs de caractéristiques de marguerite\n",
    "    descs_num = descs.shape[0] * descs.shape[1]\n",
    "    #daisy_desriptors=descs.reshape(descs_num,descs.shape[2])\n",
    "    hog_desriptor=hog(img, orientations=8, pixels_per_cell=(hog_pixels_per_cell, hog_pixels_per_cell),cells_per_block=(hog_cells_per_block, hog_cells_per_block), visualise=False,feature_vector=True)\n",
    "    return hog_desriptor\n",
    "\n",
    "\n",
    "base_path=\"SceneClass13/\" #Chemin où les données d'image sont conservées\n",
    "img_width=300\n",
    "img_height=250\n",
    "hog_pixels_per_cell=16\n",
    "orientations=8\n",
    "#Chargez les noms de fichiers correspondant à chaque catégorie de scène en listes\n",
    "category_names=listdir(base_path) ##\n",
    "for i in range(len(category_names)):\n",
    "    print (category_names[i],'=',i)\n",
    "print ('total categories:',len(category_names))\n",
    "dataset_filenames=[] #Liste pour garder le chemin de tous les fichiers dans la base de données\n",
    "dataset_labels=[]\n",
    "##category_names[0] Liste la représentation textuelle de l'identifiant de catégorie\n",
    "for category in category_names:\n",
    "    category_filenames=get_filenames(base_path+category+\"/\")##get all the filenames in that category\n",
    "    category_labels=np.ones(len(category_filenames))*category_names.index(category) ##label the category with its index position\n",
    "    dataset_filenames=dataset_filenames+category_filenames\n",
    "    dataset_labels=dataset_labels+list(category_labels)\n",
    "    #for num in range(0,100):\n",
    "    #    train_filenames = train_filenames + \n",
    "print ('Taille totale du jeu de données:',len(dataset_filenames))\n",
    "#Split into training files and testing files\n",
    "#sss = StratifiedShuffleSplit(dataset_labels,train_size=100, random_state=0)\n",
    "#train_index, test_index = [s for s in sss.split(dataset_filenames,dataset_labels)][0]\n",
    "#train_filenames, test_filenames, train_labels, test_labels = dataset_filenames[train_index, :], dataset_labels[train_index], dataset_filenames[test_index, :], dataset_labels[test_index]\n",
    "#for train_index, test_index in sss:\n",
    "#    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#    train_filenames, test_filenames = dataset_filenames[train_index], dataset_filenames[test_index]\n",
    "#    train_labels, test_labels = dataset_labels[train_index], dataset_labels[test_index]\n",
    "print ('Total des fichiers du jeu de données :',len(dataset_filenames))\n",
    "print ('Total des labels dans le jeu de données:',len(dataset_labels))\n",
    "train_filenames, test_filenames, train_labels, test_labels = train_test_split(dataset_filenames,dataset_labels,train_size=1300, stratify=dataset_labels)\n",
    "#train_filenames,test_filenames,train_labels,test_labels=train_test_split(dataset_filenames,dataset_labels,train_size =100,stratify=dataset_labels)\n",
    "print ('Nombre total des fichiers du split entrainement:',len(train_filenames))\n",
    "print ('Nombre total des fichiers du split de test:',len(test_filenames))\n",
    "#Extraire les fonctionnalités de la division de données de formation pour le traitement en aval, prend environ 12 minutes pour un ordinateur portable standard\n",
    "training_data_feature_map={} #map pour stocker les caractéristiques DAISY ainsi que la caractéristique HOG pour tous les points de données de entrainement\n",
    "daisy_descriptor_list=[] #Liste pour stocker tous les descripteurs DAISY pour former notre vocabulaire visuel en regroupant\n",
    "counter=0\n",
    "\n",
    "#Maintenant, pour former des \"mots visuels\", nous agrandissons les caractéristiques de DAISY pour former un vocabulaire, nous formons une caractéristique d'histogramme (histogramme de DAISY) correspondant à chaque caractéristique des dimensions 'number_of_clusters'\n",
    "\n",
    "#Entrée: liste des caractéristiques DAISY  et nombre de clusters\n",
    "#Sorite: a trained cluster model which will allow to get the cluster id corresponding to any input daisy feature\n",
    "def cluster_daisy_features(daisy_feature_list,number_of_clusters):\n",
    "    #km=KMeans(n_clusters=number_of_clusters)\n",
    "    km=MiniBatchKMeans(n_clusters=number_of_clusters,batch_size=number_of_clusters*10)\n",
    "    km.fit(daisy_feature_list)\n",
    "    return km\n",
    "# cacher les  warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#Le nombre de clusters est défini comme 600 #, prend plusieurs minutes pour fonctionner sur un ordinateur  standard\n",
    "filename1 = 'kmean.pkl'\n",
    "\n",
    "if os.path.exists(filename1):\n",
    "   # load the model from disk\n",
    "    print ('...Chargement du fichier de kmean...')\n",
    "    daisy_cluster_model = joblib.load(filename1)\n",
    "else:\n",
    "    for fname in tqdm(train_filenames):\n",
    "        daisy_features,hog_feature=extract_daisy_and_hog_features_from_image(fname,daisy_step_size=8,daisy_radius=8)\n",
    "    #Extraire les fonctionnalités DAISY et les fonctions HOG de l'image et enregistrer dans une map\n",
    "        training_data_feature_map[fname]=[daisy_features,hog_feature]\n",
    "        daisy_descriptor_list=daisy_descriptor_list+list(daisy_features)\n",
    "    print ('Total daisy descriptors:',len(daisy_descriptor_list))\n",
    "    print ('...Enregistrement du modèle...')\n",
    "    daisy_cluster_model=cluster_daisy_features(daisy_descriptor_list,600) \n",
    "    daisy_cluster_model.n_clusters\n",
    "    joblib.dump(daisy_cluster_model, filename1)\n",
    "\n",
    "#Fonction pour extraire la fonction hybride des images en regroupant l'histogramme de DAISY et le descripteur de HOG après l'individu\n",
    "def extract_hog_feature_from_image(fname,daisy_cluster_model):\n",
    "    #Dans le cas où nous aurions rencontré le fichier lors de l'entrainement, les caractéristiques DAISY et HOG auraient déjà été calculées\n",
    "    if fname in training_data_feature_map:\n",
    "        daisy_features=training_data_feature_map[fname][0]\n",
    "        hog_feature=training_data_feature_map[fname][1]\n",
    "    else:\n",
    "        daisy_features=extract_daisy_from_image(fname,daisy_step_size=8,daisy_radius=8)\n",
    "        \n",
    "    # Indiquer à quoi appartiennent les cluster de chaque caractéristique DAISY\n",
    "    img_clusters=daisy_cluster_model.predict(daisy_features) \n",
    "    cluster_freq_counts=pd.DataFrame(img_clusters,columns=['cnt'])['cnt'].value_counts()\n",
    "    bovw_vector=np.zeros(daisy_cluster_model.n_clusters) ##feature vector of size as the total number of clusters\n",
    "    for key in cluster_freq_counts.keys():\n",
    "        bovw_vector[key]=cluster_freq_counts[key]\n",
    "\n",
    "    #bovw_feature=bovw_vector/np.linalg.norm(bovw_vector)\n",
    "    hog_feature=hog_feature/np.linalg.norm(hog_feature)\n",
    "    return list(hog_feature)\n",
    "\n",
    "def plot_file(fname):\n",
    "    img_data=plt.imread(fname)\n",
    "    plt.imshow(rgb2gray(img_data),cmap='Greys_r')\n",
    "#Extraction de caractéristiques de données d'entrainement\n",
    "XTRAIN=[]\n",
    "YTRAIN=[]\n",
    "filename = 'trainedsvm.pkl'\n",
    "\n",
    "if os.path.exists(filename):\n",
    "   # load the model from disk\n",
    "    print ('...Chargement du fichier apprentissage...')\n",
    "    classifier = joblib.load(filename)\n",
    "else:\n",
    "    for i in tqdm(range(len(train_filenames))):\n",
    "        XTRAIN.append(extract_hog_feature_from_image(train_filenames[i],daisy_cluster_model))\n",
    "        YTRAIN.append(train_labels[i])\n",
    "    classifier=svm.SVC(C=10**1.6794140624999994, gamma=10**-0.1630955304365928, decision_function_shape='ovo') #paramètres de la fonction noyau SVM\n",
    "    print ('...Enregistrement du modèle...')\n",
    "    classifier.fit(XTRAIN,YTRAIN)\n",
    "    print ('...Enregistrement du fichier apprentissage...')\n",
    "    joblib.dump(classifier, filename)\n",
    "\n",
    "# Montrer un example de classification\n",
    "plot_file(test_filenames[4])\n",
    "print ('true label:',test_labels[4])\n",
    "feature_vector=extract_daisy_feature_from_image(test_filenames[4],daisy_cluster_model)\n",
    "print ('prediction:',classifier.predict([feature_vector]))\n",
    "\n",
    "## Test de l'extraction des fonctions de données, ne faites que pour quelques instants si un test rapide est nécessaire\n",
    "XTEST=[]\n",
    "YTEST=[]\n",
    "for i in tqdm(range(len(test_filenames))):\n",
    "    XTEST.append(extract_hog_feature_from_image(test_filenames[i],daisy_cluster_model))\n",
    "    YTEST.append(test_labels[i])\n",
    "print ('Métriques du classifieur hybride')\n",
    "print ('Nombre instances test:',len(XTEST),len(YTEST))\n",
    "#Rapport de précision\n",
    "pred=classifier.predict(XTEST)\n",
    "       \n",
    "print ('Précision globale: \\n',accuracy_score(YTEST,pred))\n",
    "print (classification_report(YTEST, pred, target_names=category_names))\n",
    "print ('Matrice de confusion:\\n')\n",
    "pd.DataFrame(confusion_matrix(YTEST,pred),\n",
    "            columns=['MITinsidecity', 'bedroom', 'PARoffice', 'MITmountain', 'MITtallbuilding', 'MIThighway', 'MITcoast', 'livingroom', 'MITopencountry', 'MITstreet', 'MITforest', 'kitchen', 'CALsuburb'],\n",
    "               index=['MITinsidecity', 'bedroom', 'PARoffice', 'MITmountain', 'MITtallbuilding', 'MIThighway', 'MITcoast', 'livingroom', 'MITopencountry', 'MITstreet', 'MITforest', 'kitchen', 'CALsuburb']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le jeu de données est déjà téléchargé et extrait!\n",
      "bedroom = 0\n",
      "CALsuburb = 1\n",
      "kitchen = 2\n",
      "livingroom = 3\n",
      "MITcoast = 4\n",
      "MITforest = 5\n",
      "MIThighway = 6\n",
      "MITinsidecity = 7\n",
      "MITmountain = 8\n",
      "MITopencountry = 9\n",
      "MITstreet = 10\n",
      "MITtallbuilding = 11\n",
      "PARoffice = 12\n",
      "total categories: 13\n",
      "Taille totale du jeu de données: 3859\n",
      "Total des fichiers du jeu de données : 3859\n",
      "Total des labels dans le jeu de données: 3859\n",
      "Nombre total des fichiers du split entrainement: 1300\n",
      "Nombre total des fichiers du split de test: 2559\n",
      "...Chargement du fichier de kmean...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/1300 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incorrect number of features. Got 2160 features, expected 136",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7fb18b0a2245>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_filenames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mXTRAIN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_hog_feature_from_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_filenames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdaisy_cluster_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m         \u001b[0mYTRAIN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[0mclassifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m1.6794140624999994\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m0.1630955304365928\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecision_function_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ovo'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#paramètres de la fonction noyau SVM\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-7fb18b0a2245>\u001b[0m in \u001b[0;36mextract_hog_feature_from_image\u001b[1;34m(fname, daisy_cluster_model)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[1;31m# Indiquer à quoi appartiennent les cluster de chaque caractéristique DAISY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m     \u001b[0mimg_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdaisy_cluster_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdaisy_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m     \u001b[0mcluster_freq_counts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_clusters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cnt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cnt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[0mbovw_vector\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdaisy_cluster_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m##feature vector of size as the total number of clusters\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1539\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cluster_centers_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1541\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_test_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1542\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_labels_inertia_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36m_check_test_data\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    865\u001b[0m             raise ValueError(\"Incorrect number of features. \"\n\u001b[0;32m    866\u001b[0m                              \"Got %d features, expected %d\" % (\n\u001b[1;32m--> 867\u001b[1;33m                                  n_features, expected_n_features))\n\u001b[0m\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Incorrect number of features. Got 2160 features, expected 136"
     ]
    }
   ],
   "source": [
    "# %load scenereclassificationTest.py\n",
    "\n",
    "#%load scenereclassificationTest.py\n",
    "from os.path import isfile, join, exists\n",
    "import os.path\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlretrieve\n",
    "from os import listdir\n",
    "from sklearn.externals import joblib\n",
    "#Pour la division de l'ensemble de données dans un ensemble de formation (100 instances) et un ensemble de test (le reste) #\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Routines de traitement d'image pour extraction / transformation de fonctionnalités\n",
    "from skimage.feature import daisy,hog#,sift\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "import skimage\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Télécharger le jeu de données\n",
    "if not exists('SceneClass13/'):\n",
    "    if not exists('SceneClass13.rar'):\n",
    "        print ('Téléchargement de SceneClass13.rar')\n",
    "        urlretrieve ('http://vision.stanford.edu/Datasets/SceneClass13.rar')\n",
    "        print ('Téléchargement de SceneClass13.rar')\n",
    "    print ('Extraction de SceneClass13.rar')\n",
    "    zipfile = ZipFile('SceneClass13.rar', 'r')\n",
    "    zipfile.extractall('./SceneClass13')\n",
    "    zipfile.close()\n",
    "    print ('Déjà extraite SceneClass13.rar')\n",
    "else:\n",
    "    print ('Le jeu de données est déjà téléchargé et extrait!')\n",
    "\n",
    "\n",
    "#Obtenez tous les noms de fichiers (y compris le chemin complet) dans un dossier en tant que liste.\n",
    "def get_filenames(path):\n",
    "    onlyfiles = [path+f for f in listdir(path) if (isfile(join(path, f)) and (f.find(\"Thumbs.db\")==-1))]\n",
    "    return onlyfiles\n",
    "# Descripteur local SIFT \n",
    "#Fonction pour extraire les fonctionnalités de SIFT ainsi que les fonctions de HOG à partir d'une image\n",
    "#def extract_daisy_and_hog_features_from_image(file_path,daisy_step_size=32,daisy_radius=32,hog_pixels_per_cell=16,hog_cells_per_block=1):\n",
    "#    img = io.imread(file_path)\n",
    "#    img_gray = rgb2gray(img)\n",
    "#    img=skimage.transform.resize(img_gray,(300,250)) ##resize to a suitable dimension, avg size of images in the dataset\n",
    "    #original, histograms=6\n",
    "#    descs = sift(img, step=daisy_step_size, radius=daisy_radius, rings=2, histograms=8,orientations=8, visualize=False)\n",
    "    #Calculer les descripteurs de caractéristiques de marguerite\n",
    "#    descs_num = descs.shape[0] * descs.shape[1]\n",
    "#    sift_desriptors=descs.reshape(descs_num,descs.shape[2])\n",
    "#  \n",
    "#    return sift_desriptors,hog_desriptor\n",
    "\n",
    "#Fonction pour extraire les fonctionnalités de HOG\n",
    "def extract_hog_from_image(file_path,daisy_step_size=32,daisy_radius=32,hog_pixels_per_cell=16,hog_cells_per_block=1):\n",
    "    img = io.imread(file_path)\n",
    "    img_gray = rgb2gray(img)\n",
    "    img=skimage.transform.resize(img_gray,(300,250)) ##resize to a suitable dimension, avg size of images in the dataset\n",
    "    #original, histograms=6\n",
    "    descs = daisy(img, step=daisy_step_size, radius=daisy_radius, rings=2, histograms=8,orientations=8, visualize=False)\n",
    "    #Calculer les descripteurs de caractéristiques de marguerite\n",
    "    descs_num = descs.shape[0] * descs.shape[1]\n",
    "    #daisy_desriptors=descs.reshape(descs_num,descs.shape[2])\n",
    "    hog_desriptor=hog(img, orientations=8, pixels_per_cell=(hog_pixels_per_cell, hog_pixels_per_cell),cells_per_block=(hog_cells_per_block, hog_cells_per_block), visualise=False,feature_vector=True)\n",
    "    return hog_desriptor\n",
    "\n",
    "\n",
    "base_path=\"SceneClass13/\" #Chemin où les données d'image sont conservées\n",
    "img_width=300\n",
    "img_height=250\n",
    "hog_pixels_per_cell=16\n",
    "orientations=8\n",
    "#Chargez les noms de fichiers correspondant à chaque catégorie de scène en listes\n",
    "category_names=listdir(base_path) ##\n",
    "for i in range(len(category_names)):\n",
    "    print (category_names[i],'=',i)\n",
    "print ('total categories:',len(category_names))\n",
    "dataset_filenames=[] #Liste pour garder le chemin de tous les fichiers dans la base de données\n",
    "dataset_labels=[]\n",
    "##category_names[0] Liste la représentation textuelle de l'identifiant de catégorie\n",
    "for category in category_names:\n",
    "    category_filenames=get_filenames(base_path+category+\"/\")##get all the filenames in that category\n",
    "    category_labels=np.ones(len(category_filenames))*category_names.index(category) ##label the category with its index position\n",
    "    dataset_filenames=dataset_filenames+category_filenames\n",
    "    dataset_labels=dataset_labels+list(category_labels)\n",
    "    #for num in range(0,100):\n",
    "    #    train_filenames = train_filenames + \n",
    "print ('Taille totale du jeu de données:',len(dataset_filenames))\n",
    "#Split into training files and testing files\n",
    "#sss = StratifiedShuffleSplit(dataset_labels,train_size=100, random_state=0)\n",
    "#train_index, test_index = [s for s in sss.split(dataset_filenames,dataset_labels)][0]\n",
    "#train_filenames, test_filenames, train_labels, test_labels = dataset_filenames[train_index, :], dataset_labels[train_index], dataset_filenames[test_index, :], dataset_labels[test_index]\n",
    "#for train_index, test_index in sss:\n",
    "#    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#    train_filenames, test_filenames = dataset_filenames[train_index], dataset_filenames[test_index]\n",
    "#    train_labels, test_labels = dataset_labels[train_index], dataset_labels[test_index]\n",
    "print ('Total des fichiers du jeu de données :',len(dataset_filenames))\n",
    "print ('Total des labels dans le jeu de données:',len(dataset_labels))\n",
    "train_filenames, test_filenames, train_labels, test_labels = train_test_split(dataset_filenames,dataset_labels,train_size=1300, stratify=dataset_labels)\n",
    "#train_filenames,test_filenames,train_labels,test_labels=train_test_split(dataset_filenames,dataset_labels,train_size =100,stratify=dataset_labels)\n",
    "print ('Nombre total des fichiers du split entrainement:',len(train_filenames))\n",
    "print ('Nombre total des fichiers du split de test:',len(test_filenames))\n",
    "#Extraire les fonctionnalités de la division de données de formation pour le traitement en aval, prend environ 12 minutes pour un ordinateur portable standard\n",
    "training_data_feature_map={} #map pour stocker les caractéristiques DAISY ainsi que la caractéristique HOG pour tous les points de données de entrainement\n",
    "daisy_descriptor_list=[] #Liste pour stocker tous les descripteurs DAISY pour former notre vocabulaire visuel en regroupant\n",
    "counter=0\n",
    "\n",
    "#Maintenant, pour former des \"mots visuels\", nous agrandissons les caractéristiques de DAISY pour former un vocabulaire, nous formons une caractéristique d'histogramme (histogramme de DAISY) correspondant à chaque caractéristique des dimensions 'number_of_clusters'\n",
    "\n",
    "#Entrée: liste des caractéristiques DAISY  et nombre de clusters\n",
    "#Sorite: a trained cluster model which will allow to get the cluster id corresponding to any input daisy feature\n",
    "def cluster_daisy_features(daisy_feature_list,number_of_clusters):\n",
    "    #km=KMeans(n_clusters=number_of_clusters)\n",
    "    km=MiniBatchKMeans(n_clusters=number_of_clusters,batch_size=number_of_clusters*10)\n",
    "    km.fit(daisy_feature_list)\n",
    "    return km\n",
    "# cacher les  warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#Le nombre de clusters est défini comme 600 #, prend plusieurs minutes pour fonctionner sur un ordinateur  standard\n",
    "filename1 = 'kmean.pkl'\n",
    "\n",
    "if os.path.exists(filename1):\n",
    "   # load the model from disk\n",
    "    print ('...Chargement du fichier de kmean...')\n",
    "    daisy_cluster_model = joblib.load(filename1)\n",
    "else:\n",
    "    for fname in tqdm(train_filenames):\n",
    "        daisy_features,hog_feature=extract_daisy_and_hog_features_from_image(fname,daisy_step_size=8,daisy_radius=8)\n",
    "    #Extraire les fonctionnalités DAISY et les fonctions HOG de l'image et enregistrer dans une map\n",
    "        training_data_feature_map[fname]=[daisy_features,hog_feature]\n",
    "        daisy_descriptor_list=daisy_descriptor_list+list(daisy_features)\n",
    "    print ('Total daisy descriptors:',len(daisy_descriptor_list))\n",
    "    print ('...Enregistrement du modèle...')\n",
    "    daisy_cluster_model=cluster_daisy_features(daisy_descriptor_list,600) \n",
    "    daisy_cluster_model.n_clusters\n",
    "    joblib.dump(daisy_cluster_model, filename1)\n",
    "\n",
    "#Fonction pour extraire la fonction hybride des images en regroupant l'histogramme de DAISY et le descripteur de HOG après l'individu\n",
    "def extract_hog_feature_from_image(fname,daisy_cluster_model):\n",
    "    #Dans le cas où nous aurions rencontré le fichier lors de l'entrainement, les caractéristiques DAISY et HOG auraient déjà été calculées\n",
    "    if fname in training_data_feature_map:\n",
    "        daisy_features=training_data_feature_map[fname][0]\n",
    "        hog_feature=training_data_feature_map[fname][1]\n",
    "    else:\n",
    "        daisy_features=extract_hog_from_image(fname,daisy_step_size=8,daisy_radius=8)\n",
    "        \n",
    "    # Indiquer à quoi appartiennent les cluster de chaque caractéristique DAISY\n",
    "    img_clusters=daisy_cluster_model.predict(daisy_features) \n",
    "    cluster_freq_counts=pd.DataFrame(img_clusters,columns=['cnt'])['cnt'].value_counts()\n",
    "    bovw_vector=np.zeros(daisy_cluster_model.n_clusters) ##feature vector of size as the total number of clusters\n",
    "    for key in cluster_freq_counts.keys():\n",
    "        bovw_vector[key]=cluster_freq_counts[key]\n",
    "\n",
    "    #bovw_feature=bovw_vector/np.linalg.norm(bovw_vector)\n",
    "    hog_feature=hog_feature/np.linalg.norm(hog_feature)\n",
    "    return list(hog_feature)\n",
    "\n",
    "def plot_file(fname):\n",
    "    img_data=plt.imread(fname)\n",
    "    plt.imshow(rgb2gray(img_data),cmap='Greys_r')\n",
    "#Extraction de caractéristiques de données d'entrainement\n",
    "XTRAIN=[]\n",
    "YTRAIN=[]\n",
    "filename = 'trainedsvm.pkl'\n",
    "\n",
    "if os.path.exists(filename):\n",
    "   # load the model from disk\n",
    "    print ('...Chargement du fichier apprentissage...')\n",
    "    classifier = joblib.load(filename)\n",
    "else:\n",
    "    for i in tqdm(range(len(train_filenames))):\n",
    "        XTRAIN.append(extract_hog_feature_from_image(train_filenames[i],daisy_cluster_model))\n",
    "        YTRAIN.append(train_labels[i])\n",
    "    classifier=svm.SVC(C=10**1.6794140624999994, gamma=10**-0.1630955304365928, decision_function_shape='ovo') #paramètres de la fonction noyau SVM\n",
    "    print ('...Enregistrement du modèle...')\n",
    "    classifier.fit(XTRAIN,YTRAIN)\n",
    "    print ('...Enregistrement du fichier apprentissage...')\n",
    "    joblib.dump(classifier, filename)\n",
    "\n",
    "# Montrer un example de classification\n",
    "plot_file(test_filenames[4])\n",
    "print ('true label:',test_labels[4])\n",
    "feature_vector=extract_daisy_feature_from_image(test_filenames[4],daisy_cluster_model)\n",
    "print ('prediction:',classifier.predict([feature_vector]))\n",
    "\n",
    "## Test de l'extraction des fonctions de données, ne faites que pour quelques instants si un test rapide est nécessaire\n",
    "XTEST=[]\n",
    "YTEST=[]\n",
    "for i in tqdm(range(len(test_filenames))):\n",
    "    XTEST.append(extract_hog_feature_from_image(test_filenames[i],daisy_cluster_model))\n",
    "    YTEST.append(test_labels[i])\n",
    "print ('Métriques du classifieur hybride')\n",
    "print ('Nombre instances test:',len(XTEST),len(YTEST))\n",
    "#Rapport de précision\n",
    "pred=classifier.predict(XTEST)\n",
    "       \n",
    "print ('Précision globale: \\n',accuracy_score(YTEST,pred))\n",
    "print (classification_report(YTEST, pred, target_names=category_names))\n",
    "print ('Matrice de confusion:\\n')\n",
    "pd.DataFrame(confusion_matrix(YTEST,pred),\n",
    "            columns=['MITinsidecity', 'bedroom', 'PARoffice', 'MITmountain', 'MITtallbuilding', 'MIThighway', 'MITcoast', 'livingroom', 'MITopencountry', 'MITstreet', 'MITforest', 'kitchen', 'CALsuburb'],\n",
    "               index=['MITinsidecity', 'bedroom', 'PARoffice', 'MITmountain', 'MITtallbuilding', 'MIThighway', 'MITcoast', 'livingroom', 'MITopencountry', 'MITstreet', 'MITforest', 'kitchen', 'CALsuburb']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le jeu de données est déjà téléchargé et extrait!\n",
      "bedroom = 0\n",
      "CALsuburb = 1\n",
      "kitchen = 2\n",
      "livingroom = 3\n",
      "MITcoast = 4\n",
      "MITforest = 5\n",
      "MIThighway = 6\n",
      "MITinsidecity = 7\n",
      "MITmountain = 8\n",
      "MITopencountry = 9\n",
      "MITstreet = 10\n",
      "MITtallbuilding = 11\n",
      "PARoffice = 12\n",
      "total categories: 13\n",
      "Taille totale du jeu de données: 3859\n",
      "Total des fichiers du jeu de données : 3859\n",
      "Total des labels dans le jeu de données: 3859\n",
      "Nombre total des fichiers du split entrainement: 1300\n",
      "Nombre total des fichiers du split de test: 2559\n",
      "...Chargement du fichier de kmean...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/1300 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incorrect number of features. Got 2160 features, expected 136",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-7fb18b0a2245>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_filenames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mXTRAIN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_hog_feature_from_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_filenames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdaisy_cluster_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m         \u001b[0mYTRAIN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[0mclassifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m1.6794140624999994\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m0.1630955304365928\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecision_function_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ovo'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#paramètres de la fonction noyau SVM\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-7fb18b0a2245>\u001b[0m in \u001b[0;36mextract_hog_feature_from_image\u001b[1;34m(fname, daisy_cluster_model)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[1;31m# Indiquer à quoi appartiennent les cluster de chaque caractéristique DAISY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m     \u001b[0mimg_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdaisy_cluster_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdaisy_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m     \u001b[0mcluster_freq_counts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_clusters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cnt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cnt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[0mbovw_vector\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdaisy_cluster_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m##feature vector of size as the total number of clusters\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1539\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cluster_centers_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1541\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_test_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1542\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_labels_inertia_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36m_check_test_data\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    865\u001b[0m             raise ValueError(\"Incorrect number of features. \"\n\u001b[0;32m    866\u001b[0m                              \"Got %d features, expected %d\" % (\n\u001b[1;32m--> 867\u001b[1;33m                                  n_features, expected_n_features))\n\u001b[0m\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Incorrect number of features. Got 2160 features, expected 136"
     ]
    }
   ],
   "source": [
    "# %load scenereclassificationTest.py\n",
    "\n",
    "#%load scenereclassificationTest.py\n",
    "from os.path import isfile, join, exists\n",
    "import os.path\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlretrieve\n",
    "from os import listdir\n",
    "from sklearn.externals import joblib\n",
    "#Pour la division de l'ensemble de données dans un ensemble de formation (100 instances) et un ensemble de test (le reste) #\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Routines de traitement d'image pour extraction / transformation de fonctionnalités\n",
    "from skimage.feature import daisy,hog#,sift\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "import skimage\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Télécharger le jeu de données\n",
    "if not exists('SceneClass13/'):\n",
    "    if not exists('SceneClass13.rar'):\n",
    "        print ('Téléchargement de SceneClass13.rar')\n",
    "        urlretrieve ('http://vision.stanford.edu/Datasets/SceneClass13.rar')\n",
    "        print ('Téléchargement de SceneClass13.rar')\n",
    "    print ('Extraction de SceneClass13.rar')\n",
    "    zipfile = ZipFile('SceneClass13.rar', 'r')\n",
    "    zipfile.extractall('./SceneClass13')\n",
    "    zipfile.close()\n",
    "    print ('Déjà extraite SceneClass13.rar')\n",
    "else:\n",
    "    print ('Le jeu de données est déjà téléchargé et extrait!')\n",
    "\n",
    "\n",
    "#Obtenez tous les noms de fichiers (y compris le chemin complet) dans un dossier en tant que liste.\n",
    "def get_filenames(path):\n",
    "    onlyfiles = [path+f for f in listdir(path) if (isfile(join(path, f)) and (f.find(\"Thumbs.db\")==-1))]\n",
    "    return onlyfiles\n",
    "# Descripteur local SIFT \n",
    "#Fonction pour extraire les fonctionnalités de SIFT ainsi que les fonctions de HOG à partir d'une image\n",
    "#def extract_daisy_and_hog_features_from_image(file_path,daisy_step_size=32,daisy_radius=32,hog_pixels_per_cell=16,hog_cells_per_block=1):\n",
    "#    img = io.imread(file_path)\n",
    "#    img_gray = rgb2gray(img)\n",
    "#    img=skimage.transform.resize(img_gray,(300,250)) ##resize to a suitable dimension, avg size of images in the dataset\n",
    "    #original, histograms=6\n",
    "#    descs = sift(img, step=daisy_step_size, radius=daisy_radius, rings=2, histograms=8,orientations=8, visualize=False)\n",
    "    #Calculer les descripteurs de caractéristiques de marguerite\n",
    "#    descs_num = descs.shape[0] * descs.shape[1]\n",
    "#    sift_desriptors=descs.reshape(descs_num,descs.shape[2])\n",
    "#  \n",
    "#    return sift_desriptors,hog_desriptor\n",
    "\n",
    "#Fonction pour extraire les fonctionnalités de HOG\n",
    "def extract_hog_from_image(file_path,daisy_step_size=32,daisy_radius=32,hog_pixels_per_cell=16,hog_cells_per_block=1):\n",
    "    img = io.imread(file_path)\n",
    "    img_gray = rgb2gray(img)\n",
    "    img=skimage.transform.resize(img_gray,(300,250)) ##resize to a suitable dimension, avg size of images in the dataset\n",
    "    #original, histograms=6\n",
    "    descs = daisy(img, step=daisy_step_size, radius=daisy_radius, rings=2, histograms=8,orientations=8, visualize=False)\n",
    "    #Calculer les descripteurs de caractéristiques de marguerite\n",
    "    descs_num = descs.shape[0] * descs.shape[1]\n",
    "    #daisy_desriptors=descs.reshape(descs_num,descs.shape[2])\n",
    "    hog_desriptor=hog(img, orientations=8, pixels_per_cell=(hog_pixels_per_cell, hog_pixels_per_cell),cells_per_block=(hog_cells_per_block, hog_cells_per_block), visualise=False,feature_vector=True)\n",
    "    return hog_desriptor\n",
    "\n",
    "\n",
    "base_path=\"SceneClass13/\" #Chemin où les données d'image sont conservées\n",
    "img_width=300\n",
    "img_height=250\n",
    "hog_pixels_per_cell=16\n",
    "orientations=8\n",
    "#Chargez les noms de fichiers correspondant à chaque catégorie de scène en listes\n",
    "category_names=listdir(base_path) ##\n",
    "for i in range(len(category_names)):\n",
    "    print (category_names[i],'=',i)\n",
    "print ('total categories:',len(category_names))\n",
    "dataset_filenames=[] #Liste pour garder le chemin de tous les fichiers dans la base de données\n",
    "dataset_labels=[]\n",
    "##category_names[0] Liste la représentation textuelle de l'identifiant de catégorie\n",
    "for category in category_names:\n",
    "    category_filenames=get_filenames(base_path+category+\"/\")##get all the filenames in that category\n",
    "    category_labels=np.ones(len(category_filenames))*category_names.index(category) ##label the category with its index position\n",
    "    dataset_filenames=dataset_filenames+category_filenames\n",
    "    dataset_labels=dataset_labels+list(category_labels)\n",
    "    #for num in range(0,100):\n",
    "    #    train_filenames = train_filenames + \n",
    "print ('Taille totale du jeu de données:',len(dataset_filenames))\n",
    "#Split into training files and testing files\n",
    "#sss = StratifiedShuffleSplit(dataset_labels,train_size=100, random_state=0)\n",
    "#train_index, test_index = [s for s in sss.split(dataset_filenames,dataset_labels)][0]\n",
    "#train_filenames, test_filenames, train_labels, test_labels = dataset_filenames[train_index, :], dataset_labels[train_index], dataset_filenames[test_index, :], dataset_labels[test_index]\n",
    "#for train_index, test_index in sss:\n",
    "#    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#    train_filenames, test_filenames = dataset_filenames[train_index], dataset_filenames[test_index]\n",
    "#    train_labels, test_labels = dataset_labels[train_index], dataset_labels[test_index]\n",
    "print ('Total des fichiers du jeu de données :',len(dataset_filenames))\n",
    "print ('Total des labels dans le jeu de données:',len(dataset_labels))\n",
    "train_filenames, test_filenames, train_labels, test_labels = train_test_split(dataset_filenames,dataset_labels,train_size=1300, stratify=dataset_labels)\n",
    "#train_filenames,test_filenames,train_labels,test_labels=train_test_split(dataset_filenames,dataset_labels,train_size =100,stratify=dataset_labels)\n",
    "print ('Nombre total des fichiers du split entrainement:',len(train_filenames))\n",
    "print ('Nombre total des fichiers du split de test:',len(test_filenames))\n",
    "#Extraire les fonctionnalités de la division de données de formation pour le traitement en aval, prend environ 12 minutes pour un ordinateur portable standard\n",
    "training_data_feature_map={} #map pour stocker les caractéristiques DAISY ainsi que la caractéristique HOG pour tous les points de données de entrainement\n",
    "daisy_descriptor_list=[] #Liste pour stocker tous les descripteurs DAISY pour former notre vocabulaire visuel en regroupant\n",
    "counter=0\n",
    "\n",
    "#Maintenant, pour former des \"mots visuels\", nous agrandissons les caractéristiques de DAISY pour former un vocabulaire, nous formons une caractéristique d'histogramme (histogramme de DAISY) correspondant à chaque caractéristique des dimensions 'number_of_clusters'\n",
    "\n",
    "#Entrée: liste des caractéristiques DAISY  et nombre de clusters\n",
    "#Sorite: a trained cluster model which will allow to get the cluster id corresponding to any input daisy feature\n",
    "def cluster_daisy_features(daisy_feature_list,number_of_clusters):\n",
    "    #km=KMeans(n_clusters=number_of_clusters)\n",
    "    km=MiniBatchKMeans(n_clusters=number_of_clusters,batch_size=number_of_clusters*10)\n",
    "    km.fit(daisy_feature_list)\n",
    "    return km\n",
    "# cacher les  warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#Le nombre de clusters est défini comme 600 #, prend plusieurs minutes pour fonctionner sur un ordinateur  standard\n",
    "filename1 = 'kmean.pkl'\n",
    "\n",
    "if os.path.exists(filename1):\n",
    "   # load the model from disk\n",
    "    print ('...Chargement du fichier de kmean...')\n",
    "    daisy_cluster_model = joblib.load(filename1)\n",
    "else:\n",
    "    for fname in tqdm(train_filenames):\n",
    "        daisy_features,hog_feature=extract_daisy_and_hog_features_from_image(fname,daisy_step_size=8,daisy_radius=8)\n",
    "    #Extraire les fonctionnalités DAISY et les fonctions HOG de l'image et enregistrer dans une map\n",
    "        training_data_feature_map[fname]=[daisy_features,hog_feature]\n",
    "        daisy_descriptor_list=daisy_descriptor_list+list(daisy_features)\n",
    "    print ('Total daisy descriptors:',len(daisy_descriptor_list))\n",
    "    print ('...Enregistrement du modèle...')\n",
    "    daisy_cluster_model=cluster_daisy_features(daisy_descriptor_list,600) \n",
    "    daisy_cluster_model.n_clusters\n",
    "    joblib.dump(daisy_cluster_model, filename1)\n",
    "\n",
    "#Fonction pour extraire la fonction hybride des images en regroupant l'histogramme de DAISY et le descripteur de HOG après l'individu\n",
    "def extract_hog_feature_from_image(fname,daisy_cluster_model):\n",
    "    #Dans le cas où nous aurions rencontré le fichier lors de l'entrainement, les caractéristiques DAISY et HOG auraient déjà été calculées\n",
    "    if fname in training_data_feature_map:\n",
    "        daisy_features=training_data_feature_map[fname][0]\n",
    "        hog_feature=training_data_feature_map[fname][1]\n",
    "    else:\n",
    "        daisy_features=extract_hog_from_image(fname,daisy_step_size=8,daisy_radius=8)\n",
    "        \n",
    "    # Indiquer à quoi appartiennent les cluster de chaque caractéristique DAISY\n",
    "    img_clusters=daisy_cluster_model.predict(daisy_features) \n",
    "    cluster_freq_counts=pd.DataFrame(img_clusters,columns=['cnt'])['cnt'].value_counts()\n",
    "    bovw_vector=np.zeros(daisy_cluster_model.n_clusters) ##feature vector of size as the total number of clusters\n",
    "    for key in cluster_freq_counts.keys():\n",
    "        bovw_vector[key]=cluster_freq_counts[key]\n",
    "\n",
    "    #bovw_feature=bovw_vector/np.linalg.norm(bovw_vector)\n",
    "    hog_feature=hog_feature/np.linalg.norm(hog_feature)\n",
    "    return list(hog_feature)\n",
    "\n",
    "def plot_file(fname):\n",
    "    img_data=plt.imread(fname)\n",
    "    plt.imshow(rgb2gray(img_data),cmap='Greys_r')\n",
    "#Extraction de caractéristiques de données d'entrainement\n",
    "XTRAIN=[]\n",
    "YTRAIN=[]\n",
    "filename = 'trainedsvm.pkl'\n",
    "\n",
    "if os.path.exists(filename):\n",
    "   # load the model from disk\n",
    "    print ('...Chargement du fichier apprentissage...')\n",
    "    classifier = joblib.load(filename)\n",
    "else:\n",
    "    for i in tqdm(range(len(train_filenames))):\n",
    "        XTRAIN.append(extract_hog_feature_from_image(train_filenames[i],daisy_cluster_model))\n",
    "        YTRAIN.append(train_labels[i])\n",
    "    classifier=svm.SVC(C=10**1.6794140624999994, gamma=10**-0.1630955304365928, decision_function_shape='ovo') #paramètres de la fonction noyau SVM\n",
    "    print ('...Enregistrement du modèle...')\n",
    "    classifier.fit(XTRAIN,YTRAIN)\n",
    "    print ('...Enregistrement du fichier apprentissage...')\n",
    "    joblib.dump(classifier, filename)\n",
    "\n",
    "# Montrer un example de classification\n",
    "plot_file(test_filenames[4])\n",
    "print ('true label:',test_labels[4])\n",
    "feature_vector=extract_daisy_feature_from_image(test_filenames[4],daisy_cluster_model)\n",
    "print ('prediction:',classifier.predict([feature_vector]))\n",
    "\n",
    "## Test de l'extraction des fonctions de données, ne faites que pour quelques instants si un test rapide est nécessaire\n",
    "XTEST=[]\n",
    "YTEST=[]\n",
    "for i in tqdm(range(len(test_filenames))):\n",
    "    XTEST.append(extract_hog_feature_from_image(test_filenames[i],daisy_cluster_model))\n",
    "    YTEST.append(test_labels[i])\n",
    "print ('Métriques du classifieur hybride')\n",
    "print ('Nombre instances test:',len(XTEST),len(YTEST))\n",
    "#Rapport de précision\n",
    "pred=classifier.predict(XTEST)\n",
    "       \n",
    "print ('Précision globale: \\n',accuracy_score(YTEST,pred))\n",
    "print (classification_report(YTEST, pred, target_names=category_names))\n",
    "print ('Matrice de confusion:\\n')\n",
    "pd.DataFrame(confusion_matrix(YTEST,pred),\n",
    "            columns=['MITinsidecity', 'bedroom', 'PARoffice', 'MITmountain', 'MITtallbuilding', 'MIThighway', 'MITcoast', 'livingroom', 'MITopencountry', 'MITstreet', 'MITforest', 'kitchen', 'CALsuburb'],\n",
    "               index=['MITinsidecity', 'bedroom', 'PARoffice', 'MITmountain', 'MITtallbuilding', 'MIThighway', 'MITcoast', 'livingroom', 'MITopencountry', 'MITstreet', 'MITforest', 'kitchen', 'CALsuburb']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le jeu de données est déjà téléchargé et extrait!\n",
      "bedroom = 0\n",
      "CALsuburb = 1\n",
      "kitchen = 2\n",
      "livingroom = 3\n",
      "MITcoast = 4\n",
      "MITforest = 5\n",
      "MIThighway = 6\n",
      "MITinsidecity = 7\n",
      "MITmountain = 8\n",
      "MITopencountry = 9\n",
      "MITstreet = 10\n",
      "MITtallbuilding = 11\n",
      "PARoffice = 12\n",
      "total categories: 13\n",
      "Taille totale du jeu de données: 3859\n",
      "Total des fichiers du jeu de données : 3859\n",
      "Total des labels dans le jeu de données: 3859\n",
      "Nombre total des fichiers du split entrainement: 1300\n",
      "Nombre total des fichiers du split de test: 2559\n",
      "...Chargement du fichier de kmean...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1300/1300 [07:30<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Enregistrement du modèle...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'hybrid_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-77fb5b6b5978>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[0mclassifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m1.6794140624999994\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m0.1630955304365928\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecision_function_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ovo'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#paramètres de la fonction noyau SVM\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'...Enregistrement du modèle...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m     \u001b[0mhybrid_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mYTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'...Enregistrement du fichier apprentissage...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hybrid_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "# %load scenereclassificationTest.py\n",
    "\n",
    "#%load scenereclassificationTest.py\n",
    "from os.path import isfile, join, exists\n",
    "import os.path\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlretrieve\n",
    "from os import listdir\n",
    "from sklearn.externals import joblib\n",
    "#Pour la division de l'ensemble de données dans un ensemble de formation (100 instances) et un ensemble de test (le reste) #\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Routines de traitement d'image pour extraction / transformation de fonctionnalités\n",
    "from skimage.feature import daisy,hog\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "import skimage\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Télécharger le jeu de données\n",
    "if not exists('SceneClass13/'):\n",
    "    if not exists('SceneClass13.rar'):\n",
    "        print ('Téléchargement de SceneClass13.rar')\n",
    "        urlretrieve ('http://vision.stanford.edu/Datasets/SceneClass13.rar')\n",
    "        print ('Téléchargement de SceneClass13.rar')\n",
    "    print ('Extraction de SceneClass13.rar')\n",
    "    zipfile = ZipFile('SceneClass13.rar', 'r')\n",
    "    zipfile.extractall('./SceneClass13')\n",
    "    zipfile.close()\n",
    "    print ('Déjà extraite SceneClass13.rar')\n",
    "else:\n",
    "    print ('Le jeu de données est déjà téléchargé et extrait!')\n",
    "\n",
    "\n",
    "#Obtenez tous les noms de fichiers (y compris le chemin complet) dans un dossier en tant que liste.\n",
    "def get_filenames(path):\n",
    "    onlyfiles = [path+f for f in listdir(path) if (isfile(join(path, f)) and (f.find(\"Thumbs.db\")==-1))]\n",
    "    return onlyfiles\n",
    "# Descripteur local DAISY et histogramme de gradients orientés (HOG)\n",
    "#Fonction pour extraire les fonctionnalités de DAISY ainsi que les fonctions de HOG à partir d'une image\n",
    "def extract_daisy_and_hog_features_from_image(file_path,daisy_step_size=32,daisy_radius=32,hog_pixels_per_cell=16,hog_cells_per_block=1):\n",
    "    img = io.imread(file_path)\n",
    "    img_gray = rgb2gray(img)\n",
    "    img=skimage.transform.resize(img_gray,(300,250)) ##resize to a suitable dimension, avg size of images in the dataset\n",
    "    #original, histograms=6\n",
    "    descs = daisy(img, step=daisy_step_size, radius=daisy_radius, rings=2, histograms=8,orientations=8, visualize=False)\n",
    "    #Calculer les descripteurs de caractéristiques de marguerite\n",
    "    descs_num = descs.shape[0] * descs.shape[1]\n",
    "    daisy_desriptors=descs.reshape(descs_num,descs.shape[2])\n",
    "    hog_desriptor=hog(img, orientations=8, pixels_per_cell=(hog_pixels_per_cell, hog_pixels_per_cell),cells_per_block=(hog_cells_per_block, hog_cells_per_block), visualise=False,feature_vector=True)\n",
    "    return daisy_desriptors,hog_desriptor\n",
    "\n",
    "\n",
    "base_path=\"SceneClass13/\" #Chemin où les données d'image sont conservées\n",
    "img_width=300\n",
    "img_height=250\n",
    "hog_pixels_per_cell=16\n",
    "orientations=8\n",
    "#Chargez les noms de fichiers correspondant à chaque catégorie de scène en listes\n",
    "category_names=listdir(base_path) ##\n",
    "for i in range(len(category_names)):\n",
    "    print (category_names[i],'=',i)\n",
    "print ('total categories:',len(category_names))\n",
    "dataset_filenames=[] #Liste pour garder le chemin de tous les fichiers dans la base de données\n",
    "dataset_labels=[]\n",
    "##category_names[0] Liste la représentation textuelle de l'identifiant de catégorie\n",
    "for category in category_names:\n",
    "    category_filenames=get_filenames(base_path+category+\"/\")##get all the filenames in that category\n",
    "    category_labels=np.ones(len(category_filenames))*category_names.index(category) ##label the category with its index position\n",
    "    dataset_filenames=dataset_filenames+category_filenames\n",
    "    dataset_labels=dataset_labels+list(category_labels)\n",
    "    #for num in range(0,100):\n",
    "    #    train_filenames = train_filenames + \n",
    "print ('Taille totale du jeu de données:',len(dataset_filenames))\n",
    "#Split into training files and testing files\n",
    "#sss = StratifiedShuffleSplit(dataset_labels,train_size=100, random_state=0)\n",
    "#train_index, test_index = [s for s in sss.split(dataset_filenames,dataset_labels)][0]\n",
    "#train_filenames, test_filenames, train_labels, test_labels = dataset_filenames[train_index, :], dataset_labels[train_index], dataset_filenames[test_index, :], dataset_labels[test_index]\n",
    "#for train_index, test_index in sss:\n",
    "#    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#    train_filenames, test_filenames = dataset_filenames[train_index], dataset_filenames[test_index]\n",
    "#    train_labels, test_labels = dataset_labels[train_index], dataset_labels[test_index]\n",
    "print ('Total des fichiers du jeu de données :',len(dataset_filenames))\n",
    "print ('Total des labels dans le jeu de données:',len(dataset_labels))\n",
    "train_filenames, test_filenames, train_labels, test_labels = train_test_split(dataset_filenames,dataset_labels,train_size=1300, stratify=dataset_labels)\n",
    "#train_filenames,test_filenames,train_labels,test_labels=train_test_split(dataset_filenames,dataset_labels,train_size =100,stratify=dataset_labels)\n",
    "print ('Nombre total des fichiers du split entrainement:',len(train_filenames))\n",
    "print ('Nombre total des fichiers du split de test:',len(test_filenames))\n",
    "#Extraire les fonctionnalités de la division de données de formation pour le traitement en aval, prend environ 12 minutes pour un ordinateur portable standard\n",
    "training_data_feature_map={} #map pour stocker les caractéristiques DAISY ainsi que la caractéristique HOG pour tous les points de données de entrainement\n",
    "daisy_descriptor_list=[] #Liste pour stocker tous les descripteurs DAISY pour former notre vocabulaire visuel en regroupant\n",
    "counter=0\n",
    "\n",
    "#Maintenant, pour former des \"mots visuels\", nous agrandissons les caractéristiques de DAISY pour former un vocabulaire, nous formons une caractéristique d'histogramme (histogramme de DAISY) correspondant à chaque caractéristique des dimensions 'number_of_clusters'\n",
    "\n",
    "#Entrée: liste des caractéristiques DAISY  et nombre de clusters\n",
    "#Sorite: a trained cluster model which will allow to get the cluster id corresponding to any input daisy feature\n",
    "def cluster_daisy_features(daisy_feature_list,number_of_clusters):\n",
    "    #km=KMeans(n_clusters=number_of_clusters)\n",
    "    km=MiniBatchKMeans(n_clusters=number_of_clusters,batch_size=number_of_clusters*10)\n",
    "    km.fit(daisy_feature_list)\n",
    "    return km\n",
    "# cacher les  warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#Le nombre de clusters est défini comme 600 #, prend plusieurs minutes pour fonctionner sur un ordinateur  standard\n",
    "filename1 = 'kmean.pkl'\n",
    "\n",
    "if os.path.exists(filename1):\n",
    "   # load the model from disk\n",
    "    print ('...Chargement du fichier de kmean...')\n",
    "    daisy_cluster_model = joblib.load(filename1)\n",
    "else:\n",
    "    for fname in tqdm(train_filenames):\n",
    "        daisy_features,hog_feature=extract_daisy_and_hog_features_from_image(fname,daisy_step_size=8,daisy_radius=8)\n",
    "    #Extraire les fonctionnalités DAISY et les fonctions HOG de l'image et enregistrer dans une map\n",
    "        training_data_feature_map[fname]=[daisy_features,hog_feature]\n",
    "        daisy_descriptor_list=daisy_descriptor_list+list(daisy_features)\n",
    "    print ('Total daisy descriptors:',len(daisy_descriptor_list))\n",
    "    print ('...Enregistrement du modèle...')\n",
    "    daisy_cluster_model=cluster_daisy_features(daisy_descriptor_list,600) \n",
    "    daisy_cluster_model.n_clusters\n",
    "    joblib.dump(daisy_cluster_model, filename1)\n",
    "\n",
    "#Fonction pour extraire la fonction hybride des images en regroupant l'histogramme de DAISY et le descripteur de HOG après l'individu\n",
    "def extract_hog_feature_from_image(fname,daisy_cluster_model):\n",
    "    #Dans le cas où nous aurions rencontré le fichier lors de l'entrainement, les caractéristiques DAISY et HOG auraient déjà été calculées\n",
    "    if fname in training_data_feature_map:\n",
    "        daisy_features=training_data_feature_map[fname][0]\n",
    "        hog_feature=training_data_feature_map[fname][1]\n",
    "    else:\n",
    "        daisy_features,hog_feature=extract_daisy_and_hog_features_from_image(fname,daisy_step_size=8,daisy_radius=8)\n",
    "        \n",
    "    # Indiquer à quoi appartiennent les cluster de chaque caractéristique DAISY\n",
    "    img_clusters=daisy_cluster_model.predict(daisy_features) \n",
    "    cluster_freq_counts=pd.DataFrame(img_clusters,columns=['cnt'])['cnt'].value_counts()\n",
    "    bovw_vector=np.zeros(daisy_cluster_model.n_clusters) ##feature vector of size as the total number of clusters\n",
    "    for key in cluster_freq_counts.keys():\n",
    "        bovw_vector[key]=cluster_freq_counts[key]\n",
    "\n",
    "    #bovw_feature=bovw_vector/np.linalg.norm(bovw_vector)\n",
    "    hog_feature=hog_feature/np.linalg.norm(hog_feature)\n",
    "    return list(hog_feature)\n",
    "\n",
    "def plot_file(fname):\n",
    "    img_data=plt.imread(fname)\n",
    "    plt.imshow(rgb2gray(img_data),cmap='Greys_r')\n",
    "#Extraction de caractéristiques de données d'entrainement\n",
    "XTRAIN=[]\n",
    "YTRAIN=[]\n",
    "filename = 'trainedsvm.pkl'\n",
    "\n",
    "if os.path.exists(filename):\n",
    "   # load the model from disk\n",
    "    print ('...Chargement du fichier apprentissage...')\n",
    "    classifier = joblib.load(filename)\n",
    "else:\n",
    "    for i in tqdm(range(len(train_filenames))):\n",
    "        XTRAIN.append(extract_hog_feature_from_image(train_filenames[i],daisy_cluster_model))\n",
    "        YTRAIN.append(train_labels[i])\n",
    "    classifier=svm.SVC(C=10**1.6794140624999994, gamma=10**-0.1630955304365928, decision_function_shape='ovo') #paramètres de la fonction noyau SVM\n",
    "    print ('...Enregistrement du modèle...')\n",
    "    hybrid_classifier.fit(XTRAIN,YTRAIN)\n",
    "    print ('...Enregistrement du fichier apprentissage...')\n",
    "    joblib.dump(classifier, filename)\n",
    "\n",
    "# Montrer un example de classification\n",
    "plot_file(test_filenames[4])\n",
    "print ('true label:',test_labels[4])\n",
    "feature_vector=extract_hog_feature_from_image(test_filenames[4],daisy_cluster_model)\n",
    "print ('prediction:',classifier.predict([feature_vector]))\n",
    "\n",
    "## Test de l'extraction des fonctions de données, ne faites que pour quelques instants si un test rapide est nécessaire\n",
    "XTEST=[]\n",
    "YTEST=[]\n",
    "for i in tqdm(range(len(test_filenames))):\n",
    "    XTEST.append(extract_hog_feature_from_image(test_filenames[i],daisy_cluster_model))\n",
    "    YTEST.append(test_labels[i])\n",
    "print ('Métriques du classifieur hybride')\n",
    "print ('Nombre instances test:',len(XTEST),len(YTEST))\n",
    "#Rapport de précision\n",
    "hybridpred=hybrid_classifier.predict(XTEST)\n",
    "       \n",
    "print ('Précision globale: \\n',accuracy_score(YTEST,hybridpred))\n",
    "print (classification_report(YTEST, hybridpred, target_names=category_names))\n",
    "print ('Matrice de confusion:\\n')\n",
    "pd.DataFrame(confusion_matrix(YTEST,hybridpred),\n",
    "            columns=['MITinsidecity', 'bedroom', 'PARoffice', 'MITmountain', 'MITtallbuilding', 'MIThighway', 'MITcoast', 'livingroom', 'MITopencountry', 'MITstreet', 'MITforest', 'kitchen', 'CALsuburb'],\n",
    "               index=['MITinsidecity', 'bedroom', 'PARoffice', 'MITmountain', 'MITtallbuilding', 'MIThighway', 'MITcoast', 'livingroom', 'MITopencountry', 'MITstreet', 'MITforest', 'kitchen', 'CALsuburb']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le jeu de données est déjà téléchargé et extrait!\n",
      "bedroom = 0\n",
      "CALsuburb = 1\n",
      "kitchen = 2\n",
      "livingroom = 3\n",
      "MITcoast = 4\n",
      "MITforest = 5\n",
      "MIThighway = 6\n",
      "MITinsidecity = 7\n",
      "MITmountain = 8\n",
      "MITopencountry = 9\n",
      "MITstreet = 10\n",
      "MITtallbuilding = 11\n",
      "PARoffice = 12\n",
      "total categories: 13\n",
      "Taille totale du jeu de données: 3859\n",
      "Total des fichiers du jeu de données : 3859\n",
      "Total des labels dans le jeu de données: 3859\n",
      "Nombre total des fichiers du split entrainement: 1300\n",
      "Nombre total des fichiers du split de test: 2559\n",
      "...Chargement du fichier de kmean...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1300/1300 [07:25<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Enregistrement du modèle...\n",
      "...Enregistrement du fichier apprentissage...\n",
      "true label: 1.0\n",
      "prediction: [ 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2559/2559 [14:16<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métriques du classifieur hybride\n",
      "Nombre instances test: 2559 2559\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'hybridpred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-3881c15186f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXTEST\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Précision globale: \\n'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mYTEST\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhybridpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mYTEST\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategory_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Matrice de confusion:\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hybridpred' is not defined"
     ]
    }
   ],
   "source": [
    "# %load scenereclassificationTest.py\n",
    "\n",
    "#%load scenereclassificationTest.py\n",
    "from os.path import isfile, join, exists\n",
    "import os.path\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlretrieve\n",
    "from os import listdir\n",
    "from sklearn.externals import joblib\n",
    "#Pour la division de l'ensemble de données dans un ensemble de formation (100 instances) et un ensemble de test (le reste) #\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Routines de traitement d'image pour extraction / transformation de fonctionnalités\n",
    "from skimage.feature import daisy,hog\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "import skimage\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Télécharger le jeu de données\n",
    "if not exists('SceneClass13/'):\n",
    "    if not exists('SceneClass13.rar'):\n",
    "        print ('Téléchargement de SceneClass13.rar')\n",
    "        urlretrieve ('http://vision.stanford.edu/Datasets/SceneClass13.rar')\n",
    "        print ('Téléchargement de SceneClass13.rar')\n",
    "    print ('Extraction de SceneClass13.rar')\n",
    "    zipfile = ZipFile('SceneClass13.rar', 'r')\n",
    "    zipfile.extractall('./SceneClass13')\n",
    "    zipfile.close()\n",
    "    print ('Déjà extraite SceneClass13.rar')\n",
    "else:\n",
    "    print ('Le jeu de données est déjà téléchargé et extrait!')\n",
    "\n",
    "\n",
    "#Obtenez tous les noms de fichiers (y compris le chemin complet) dans un dossier en tant que liste.\n",
    "def get_filenames(path):\n",
    "    onlyfiles = [path+f for f in listdir(path) if (isfile(join(path, f)) and (f.find(\"Thumbs.db\")==-1))]\n",
    "    return onlyfiles\n",
    "# Descripteur local DAISY et histogramme de gradients orientés (HOG)\n",
    "#Fonction pour extraire les fonctionnalités de DAISY ainsi que les fonctions de HOG à partir d'une image\n",
    "def extract_daisy_and_hog_features_from_image(file_path,daisy_step_size=32,daisy_radius=32,hog_pixels_per_cell=16,hog_cells_per_block=1):\n",
    "    img = io.imread(file_path)\n",
    "    img_gray = rgb2gray(img)\n",
    "    img=skimage.transform.resize(img_gray,(300,250)) ##resize to a suitable dimension, avg size of images in the dataset\n",
    "    #original, histograms=6\n",
    "    descs = daisy(img, step=daisy_step_size, radius=daisy_radius, rings=2, histograms=8,orientations=8, visualize=False)\n",
    "    #Calculer les descripteurs de caractéristiques de marguerite\n",
    "    descs_num = descs.shape[0] * descs.shape[1]\n",
    "    daisy_desriptors=descs.reshape(descs_num,descs.shape[2])\n",
    "    hog_desriptor=hog(img, orientations=8, pixels_per_cell=(hog_pixels_per_cell, hog_pixels_per_cell),cells_per_block=(hog_cells_per_block, hog_cells_per_block), visualise=False,feature_vector=True)\n",
    "    return daisy_desriptors,hog_desriptor\n",
    "\n",
    "\n",
    "base_path=\"SceneClass13/\" #Chemin où les données d'image sont conservées\n",
    "img_width=300\n",
    "img_height=250\n",
    "hog_pixels_per_cell=16\n",
    "orientations=8\n",
    "#Chargez les noms de fichiers correspondant à chaque catégorie de scène en listes\n",
    "category_names=listdir(base_path) ##\n",
    "for i in range(len(category_names)):\n",
    "    print (category_names[i],'=',i)\n",
    "print ('total categories:',len(category_names))\n",
    "dataset_filenames=[] #Liste pour garder le chemin de tous les fichiers dans la base de données\n",
    "dataset_labels=[]\n",
    "##category_names[0] Liste la représentation textuelle de l'identifiant de catégorie\n",
    "for category in category_names:\n",
    "    category_filenames=get_filenames(base_path+category+\"/\")##get all the filenames in that category\n",
    "    category_labels=np.ones(len(category_filenames))*category_names.index(category) ##label the category with its index position\n",
    "    dataset_filenames=dataset_filenames+category_filenames\n",
    "    dataset_labels=dataset_labels+list(category_labels)\n",
    "    #for num in range(0,100):\n",
    "    #    train_filenames = train_filenames + \n",
    "print ('Taille totale du jeu de données:',len(dataset_filenames))\n",
    "#Split into training files and testing files\n",
    "#sss = StratifiedShuffleSplit(dataset_labels,train_size=100, random_state=0)\n",
    "#train_index, test_index = [s for s in sss.split(dataset_filenames,dataset_labels)][0]\n",
    "#train_filenames, test_filenames, train_labels, test_labels = dataset_filenames[train_index, :], dataset_labels[train_index], dataset_filenames[test_index, :], dataset_labels[test_index]\n",
    "#for train_index, test_index in sss:\n",
    "#    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#    train_filenames, test_filenames = dataset_filenames[train_index], dataset_filenames[test_index]\n",
    "#    train_labels, test_labels = dataset_labels[train_index], dataset_labels[test_index]\n",
    "print ('Total des fichiers du jeu de données :',len(dataset_filenames))\n",
    "print ('Total des labels dans le jeu de données:',len(dataset_labels))\n",
    "train_filenames, test_filenames, train_labels, test_labels = train_test_split(dataset_filenames,dataset_labels,train_size=1300, stratify=dataset_labels)\n",
    "#train_filenames,test_filenames,train_labels,test_labels=train_test_split(dataset_filenames,dataset_labels,train_size =100,stratify=dataset_labels)\n",
    "print ('Nombre total des fichiers du split entrainement:',len(train_filenames))\n",
    "print ('Nombre total des fichiers du split de test:',len(test_filenames))\n",
    "#Extraire les fonctionnalités de la division de données de formation pour le traitement en aval, prend environ 12 minutes pour un ordinateur portable standard\n",
    "training_data_feature_map={} #map pour stocker les caractéristiques DAISY ainsi que la caractéristique HOG pour tous les points de données de entrainement\n",
    "daisy_descriptor_list=[] #Liste pour stocker tous les descripteurs DAISY pour former notre vocabulaire visuel en regroupant\n",
    "counter=0\n",
    "\n",
    "#Maintenant, pour former des \"mots visuels\", nous agrandissons les caractéristiques de DAISY pour former un vocabulaire, nous formons une caractéristique d'histogramme (histogramme de DAISY) correspondant à chaque caractéristique des dimensions 'number_of_clusters'\n",
    "\n",
    "#Entrée: liste des caractéristiques DAISY  et nombre de clusters\n",
    "#Sorite: a trained cluster model which will allow to get the cluster id corresponding to any input daisy feature\n",
    "def cluster_daisy_features(daisy_feature_list,number_of_clusters):\n",
    "    #km=KMeans(n_clusters=number_of_clusters)\n",
    "    km=MiniBatchKMeans(n_clusters=number_of_clusters,batch_size=number_of_clusters*10)\n",
    "    km.fit(daisy_feature_list)\n",
    "    return km\n",
    "# cacher les  warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#Le nombre de clusters est défini comme 600 #, prend plusieurs minutes pour fonctionner sur un ordinateur  standard\n",
    "filename1 = 'kmean.pkl'\n",
    "\n",
    "if os.path.exists(filename1):\n",
    "   # load the model from disk\n",
    "    print ('...Chargement du fichier de kmean...')\n",
    "    daisy_cluster_model = joblib.load(filename1)\n",
    "else:\n",
    "    for fname in tqdm(train_filenames):\n",
    "        daisy_features,hog_feature=extract_daisy_and_hog_features_from_image(fname,daisy_step_size=8,daisy_radius=8)\n",
    "    #Extraire les fonctionnalités DAISY et les fonctions HOG de l'image et enregistrer dans une map\n",
    "        training_data_feature_map[fname]=[daisy_features,hog_feature]\n",
    "        daisy_descriptor_list=daisy_descriptor_list+list(daisy_features)\n",
    "    print ('Total daisy descriptors:',len(daisy_descriptor_list))\n",
    "    print ('...Enregistrement du modèle...')\n",
    "    daisy_cluster_model=cluster_daisy_features(daisy_descriptor_list,600) \n",
    "    daisy_cluster_model.n_clusters\n",
    "    joblib.dump(daisy_cluster_model, filename1)\n",
    "\n",
    "#Fonction pour extraire la fonction hybride des images en regroupant l'histogramme de DAISY et le descripteur de HOG après l'individu\n",
    "def extract_hog_feature_from_image(fname,daisy_cluster_model):\n",
    "    #Dans le cas où nous aurions rencontré le fichier lors de l'entrainement, les caractéristiques DAISY et HOG auraient déjà été calculées\n",
    "    if fname in training_data_feature_map:\n",
    "        daisy_features=training_data_feature_map[fname][0]\n",
    "        hog_feature=training_data_feature_map[fname][1]\n",
    "    else:\n",
    "        daisy_features,hog_feature=extract_daisy_and_hog_features_from_image(fname,daisy_step_size=8,daisy_radius=8)\n",
    "        \n",
    "    # Indiquer à quoi appartiennent les cluster de chaque caractéristique DAISY\n",
    "    img_clusters=daisy_cluster_model.predict(daisy_features) \n",
    "    cluster_freq_counts=pd.DataFrame(img_clusters,columns=['cnt'])['cnt'].value_counts()\n",
    "    bovw_vector=np.zeros(daisy_cluster_model.n_clusters) ##feature vector of size as the total number of clusters\n",
    "    for key in cluster_freq_counts.keys():\n",
    "        bovw_vector[key]=cluster_freq_counts[key]\n",
    "\n",
    "    #bovw_feature=bovw_vector/np.linalg.norm(bovw_vector)\n",
    "    hog_feature=hog_feature/np.linalg.norm(hog_feature)\n",
    "    return list(hog_feature)\n",
    "\n",
    "def plot_file(fname):\n",
    "    img_data=plt.imread(fname)\n",
    "    plt.imshow(rgb2gray(img_data),cmap='Greys_r')\n",
    "#Extraction de caractéristiques de données d'entrainement\n",
    "XTRAIN=[]\n",
    "YTRAIN=[]\n",
    "filename = 'trainedsvm.pkl'\n",
    "\n",
    "if os.path.exists(filename):\n",
    "   # load the model from disk\n",
    "    print ('...Chargement du fichier apprentissage...')\n",
    "    classifier = joblib.load(filename)\n",
    "else:\n",
    "    for i in tqdm(range(len(train_filenames))):\n",
    "        XTRAIN.append(extract_hog_feature_from_image(train_filenames[i],daisy_cluster_model))\n",
    "        YTRAIN.append(train_labels[i])\n",
    "    classifier=svm.SVC(C=10**1.6794140624999994, gamma=10**-0.1630955304365928, decision_function_shape='ovo') #paramètres de la fonction noyau SVM\n",
    "    print ('...Enregistrement du modèle...')\n",
    "    classifier.fit(XTRAIN,YTRAIN)\n",
    "    print ('...Enregistrement du fichier apprentissage...')\n",
    "    joblib.dump(classifier, filename)\n",
    "\n",
    "# Montrer un example de classification\n",
    "plot_file(test_filenames[4])\n",
    "print ('true label:',test_labels[4])\n",
    "feature_vector=extract_hog_feature_from_image(test_filenames[4],daisy_cluster_model)\n",
    "print ('prediction:',classifier.predict([feature_vector]))\n",
    "\n",
    "## Test de l'extraction des fonctions de données, ne faites que pour quelques instants si un test rapide est nécessaire\n",
    "XTEST=[]\n",
    "YTEST=[]\n",
    "for i in tqdm(range(len(test_filenames))):\n",
    "    XTEST.append(extract_hog_feature_from_image(test_filenames[i],daisy_cluster_model))\n",
    "    YTEST.append(test_labels[i])\n",
    "print ('Métriques du classifieur hybride')\n",
    "print ('Nombre instances test:',len(XTEST),len(YTEST))\n",
    "#Rapport de précision\n",
    "pred=classifier.predict(XTEST)\n",
    "       \n",
    "print ('Précision globale: \\n',accuracy_score(YTEST,hybridpred))\n",
    "print (classification_report(YTEST, pred, target_names=category_names))\n",
    "print ('Matrice de confusion:\\n')\n",
    "pd.DataFrame(confusion_matrix(YTEST,pred),\n",
    "            columns=['MITinsidecity', 'bedroom', 'PARoffice', 'MITmountain', 'MITtallbuilding', 'MIThighway', 'MITcoast', 'livingroom', 'MITopencountry', 'MITstreet', 'MITforest', 'kitchen', 'CALsuburb'],\n",
    "               index=['MITinsidecity', 'bedroom', 'PARoffice', 'MITmountain', 'MITtallbuilding', 'MIThighway', 'MITcoast', 'livingroom', 'MITopencountry', 'MITstreet', 'MITforest', 'kitchen', 'CALsuburb']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le jeu de données est déjà téléchargé et extrait!\n",
      "bedroom = 0\n",
      "CALsuburb = 1\n",
      "kitchen = 2\n",
      "livingroom = 3\n",
      "MITcoast = 4\n",
      "MITforest = 5\n",
      "MIThighway = 6\n",
      "MITinsidecity = 7\n",
      "MITmountain = 8\n",
      "MITopencountry = 9\n",
      "MITstreet = 10\n",
      "MITtallbuilding = 11\n",
      "PARoffice = 12\n",
      "total categories: 13\n",
      "Taille totale du jeu de données: 3859\n",
      "Total des fichiers du jeu de données : 3859\n",
      "Total des labels dans le jeu de données: 3859\n",
      "Nombre total des fichiers du split entrainement: 1300\n",
      "Nombre total des fichiers du split de test: 2559\n",
      "...Chargement du fichier de kmean...\n",
      "...Chargement du fichier apprentissage...\n",
      "true label: 7.0\n",
      "prediction: [ 11.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2559/2559 [13:36<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métriques du classifieur hybride\n",
      "Nombre instances test: 2559 2559\n",
      "Précision globale: \n",
      " 0.794060179758\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        bedroom       0.65      0.64      0.64       143\n",
      "      CALsuburb       0.95      0.88      0.91       160\n",
      "        kitchen       0.69      0.59      0.64       139\n",
      "     livingroom       0.68      0.72      0.70       192\n",
      "       MITcoast       0.84      0.85      0.85       239\n",
      "      MITforest       0.85      0.92      0.88       217\n",
      "     MIThighway       0.90      0.82      0.86       172\n",
      "  MITinsidecity       0.82      0.80      0.81       204\n",
      "    MITmountain       0.85      0.80      0.82       248\n",
      " MITopencountry       0.73      0.80      0.76       272\n",
      "      MITstreet       0.86      0.86      0.86       194\n",
      "MITtallbuilding       0.74      0.83      0.78       236\n",
      "      PARoffice       0.79      0.65      0.71       143\n",
      "\n",
      "    avg / total       0.80      0.79      0.79      2559\n",
      "\n",
      "Matrice de confusion:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MITinsidecity</th>\n",
       "      <th>bedroom</th>\n",
       "      <th>PARoffice</th>\n",
       "      <th>MITmountain</th>\n",
       "      <th>MITtallbuilding</th>\n",
       "      <th>MIThighway</th>\n",
       "      <th>MITcoast</th>\n",
       "      <th>livingroom</th>\n",
       "      <th>MITopencountry</th>\n",
       "      <th>MITstreet</th>\n",
       "      <th>MITforest</th>\n",
       "      <th>kitchen</th>\n",
       "      <th>CALsuburb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MITinsidecity</th>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bedroom</th>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PARoffice</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MITmountain</th>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>139</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MITtallbuilding</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MIThighway</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>199</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MITcoast</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>livingroom</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>164</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MITopencountry</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>199</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MITstreet</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>218</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MITforest</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>166</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kitchen</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>196</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CALsuburb</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 MITinsidecity  bedroom  PARoffice  MITmountain  \\\n",
       "MITinsidecity               91        0          9           24   \n",
       "bedroom                      0      140          0            2   \n",
       "PARoffice                    5        1         82           26   \n",
       "MITmountain                 30        2          5          139   \n",
       "MITtallbuilding              1        0          0            0   \n",
       "MIThighway                   0        0          0            0   \n",
       "MITcoast                     0        0          0            0   \n",
       "livingroom                   0        1          2            1   \n",
       "MITopencountry               0        0          0            0   \n",
       "MITstreet                    0        1          0            0   \n",
       "MITforest                    1        2          2            2   \n",
       "kitchen                      2        0          2            2   \n",
       "CALsuburb                   10        1         17            9   \n",
       "\n",
       "                 MITtallbuilding  MIThighway  MITcoast  livingroom  \\\n",
       "MITinsidecity                  2           2         1           3   \n",
       "bedroom                        0           2         0           0   \n",
       "PARoffice                      0           0         0           7   \n",
       "MITmountain                    1           0         1           2   \n",
       "MITtallbuilding              204           0         2           0   \n",
       "MIThighway                     1         199         0           0   \n",
       "MITcoast                       8           0       141           0   \n",
       "livingroom                     0           0         3         164   \n",
       "MITopencountry                 5          16         1           0   \n",
       "MITstreet                     21          12         5           0   \n",
       "MITforest                      0           0         1           7   \n",
       "kitchen                        1           4         2          13   \n",
       "CALsuburb                      0           0         0           5   \n",
       "\n",
       "                 MITopencountry  MITstreet  MITforest  kitchen  CALsuburb  \n",
       "MITinsidecity                 1          0          1        3          6  \n",
       "bedroom                       1          0          8        7          0  \n",
       "PARoffice                     0          0          2        5         11  \n",
       "MITmountain                   1          0          3        6          2  \n",
       "MITtallbuilding               2         29          1        0          0  \n",
       "MIThighway                   12          5          0        0          0  \n",
       "MITcoast                      2         18          1        2          0  \n",
       "livingroom                    0          2          5       25          1  \n",
       "MITopencountry              199         22          0        5          0  \n",
       "MITstreet                    15        218          0        0          0  \n",
       "MITforest                     0          3        166       10          0  \n",
       "kitchen                       2          2          5      196          5  \n",
       "CALsuburb                     0          0          1        7         93  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load scenereclassificationTest.py\n",
    "\n",
    "#%load scenereclassificationTest.py\n",
    "from os.path import isfile, join, exists\n",
    "import os.path\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlretrieve\n",
    "from os import listdir\n",
    "from sklearn.externals import joblib\n",
    "#Pour la division de l'ensemble de données dans un ensemble de formation (100 instances) et un ensemble de test (le reste) #\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Routines de traitement d'image pour extraction / transformation de fonctionnalités\n",
    "from skimage.feature import daisy,hog\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "import skimage\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Télécharger le jeu de données\n",
    "if not exists('SceneClass13/'):\n",
    "    if not exists('SceneClass13.rar'):\n",
    "        print ('Téléchargement de SceneClass13.rar')\n",
    "        urlretrieve ('http://vision.stanford.edu/Datasets/SceneClass13.rar')\n",
    "        print ('Téléchargement de SceneClass13.rar')\n",
    "    print ('Extraction de SceneClass13.rar')\n",
    "    zipfile = ZipFile('SceneClass13.rar', 'r')\n",
    "    zipfile.extractall('./SceneClass13')\n",
    "    zipfile.close()\n",
    "    print ('Déjà extraite SceneClass13.rar')\n",
    "else:\n",
    "    print ('Le jeu de données est déjà téléchargé et extrait!')\n",
    "\n",
    "\n",
    "#Obtenez tous les noms de fichiers (y compris le chemin complet) dans un dossier en tant que liste.\n",
    "def get_filenames(path):\n",
    "    onlyfiles = [path+f for f in listdir(path) if (isfile(join(path, f)) and (f.find(\"Thumbs.db\")==-1))]\n",
    "    return onlyfiles\n",
    "# Descripteur local DAISY et histogramme de gradients orientés (HOG)\n",
    "#Fonction pour extraire les fonctionnalités de DAISY ainsi que les fonctions de HOG à partir d'une image\n",
    "def extract_daisy_and_hog_features_from_image(file_path,daisy_step_size=32,daisy_radius=32,hog_pixels_per_cell=16,hog_cells_per_block=1):\n",
    "    img = io.imread(file_path)\n",
    "    img_gray = rgb2gray(img)\n",
    "    img=skimage.transform.resize(img_gray,(300,250)) ##resize to a suitable dimension, avg size of images in the dataset\n",
    "    #original, histograms=6\n",
    "    descs = daisy(img, step=daisy_step_size, radius=daisy_radius, rings=2, histograms=8,orientations=8, visualize=False)\n",
    "    #Calculer les descripteurs de caractéristiques de marguerite\n",
    "    descs_num = descs.shape[0] * descs.shape[1]\n",
    "    daisy_desriptors=descs.reshape(descs_num,descs.shape[2])\n",
    "    hog_desriptor=hog(img, orientations=8, pixels_per_cell=(hog_pixels_per_cell, hog_pixels_per_cell),cells_per_block=(hog_cells_per_block, hog_cells_per_block), visualise=False,feature_vector=True)\n",
    "    return daisy_desriptors,hog_desriptor\n",
    "\n",
    "\n",
    "base_path=\"SceneClass13/\" #Chemin où les données d'image sont conservées\n",
    "img_width=300\n",
    "img_height=250\n",
    "hog_pixels_per_cell=16\n",
    "orientations=8\n",
    "#Chargez les noms de fichiers correspondant à chaque catégorie de scène en listes\n",
    "category_names=listdir(base_path) ##\n",
    "for i in range(len(category_names)):\n",
    "    print (category_names[i],'=',i)\n",
    "print ('total categories:',len(category_names))\n",
    "dataset_filenames=[] #Liste pour garder le chemin de tous les fichiers dans la base de données\n",
    "dataset_labels=[]\n",
    "##category_names[0] Liste la représentation textuelle de l'identifiant de catégorie\n",
    "for category in category_names:\n",
    "    category_filenames=get_filenames(base_path+category+\"/\")##get all the filenames in that category\n",
    "    category_labels=np.ones(len(category_filenames))*category_names.index(category) ##label the category with its index position\n",
    "    dataset_filenames=dataset_filenames+category_filenames\n",
    "    dataset_labels=dataset_labels+list(category_labels)\n",
    "    #for num in range(0,100):\n",
    "    #    train_filenames = train_filenames + \n",
    "print ('Taille totale du jeu de données:',len(dataset_filenames))\n",
    "#Split into training files and testing files\n",
    "#sss = StratifiedShuffleSplit(dataset_labels,train_size=100, random_state=0)\n",
    "#train_index, test_index = [s for s in sss.split(dataset_filenames,dataset_labels)][0]\n",
    "#train_filenames, test_filenames, train_labels, test_labels = dataset_filenames[train_index, :], dataset_labels[train_index], dataset_filenames[test_index, :], dataset_labels[test_index]\n",
    "#for train_index, test_index in sss:\n",
    "#    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#    train_filenames, test_filenames = dataset_filenames[train_index], dataset_filenames[test_index]\n",
    "#    train_labels, test_labels = dataset_labels[train_index], dataset_labels[test_index]\n",
    "print ('Total des fichiers du jeu de données :',len(dataset_filenames))\n",
    "print ('Total des labels dans le jeu de données:',len(dataset_labels))\n",
    "train_filenames, test_filenames, train_labels, test_labels = train_test_split(dataset_filenames,dataset_labels,train_size=1300, stratify=dataset_labels)\n",
    "#train_filenames,test_filenames,train_labels,test_labels=train_test_split(dataset_filenames,dataset_labels,train_size =100,stratify=dataset_labels)\n",
    "print ('Nombre total des fichiers du split entrainement:',len(train_filenames))\n",
    "print ('Nombre total des fichiers du split de test:',len(test_filenames))\n",
    "#Extraire les fonctionnalités de la division de données de formation pour le traitement en aval, prend environ 12 minutes pour un ordinateur portable standard\n",
    "training_data_feature_map={} #map pour stocker les caractéristiques DAISY ainsi que la caractéristique HOG pour tous les points de données de entrainement\n",
    "daisy_descriptor_list=[] #Liste pour stocker tous les descripteurs DAISY pour former notre vocabulaire visuel en regroupant\n",
    "counter=0\n",
    "\n",
    "#Maintenant, pour former des \"mots visuels\", nous agrandissons les caractéristiques de DAISY pour former un vocabulaire, nous formons une caractéristique d'histogramme (histogramme de DAISY) correspondant à chaque caractéristique des dimensions 'number_of_clusters'\n",
    "\n",
    "#Entrée: liste des caractéristiques DAISY  et nombre de clusters\n",
    "#Sorite: a trained cluster model which will allow to get the cluster id corresponding to any input daisy feature\n",
    "def cluster_daisy_features(daisy_feature_list,number_of_clusters):\n",
    "    #km=KMeans(n_clusters=number_of_clusters)\n",
    "    km=MiniBatchKMeans(n_clusters=number_of_clusters,batch_size=number_of_clusters*10)\n",
    "    km.fit(daisy_feature_list)\n",
    "    return km\n",
    "# cacher les  warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#Le nombre de clusters est défini comme 600 #, prend plusieurs minutes pour fonctionner sur un ordinateur  standard\n",
    "filename1 = 'kmean.pkl'\n",
    "\n",
    "if os.path.exists(filename1):\n",
    "   # load the model from disk\n",
    "    print ('...Chargement du fichier de kmean...')\n",
    "    daisy_cluster_model = joblib.load(filename1)\n",
    "else:\n",
    "    for fname in tqdm(train_filenames):\n",
    "        daisy_features,hog_feature=extract_daisy_and_hog_features_from_image(fname,daisy_step_size=8,daisy_radius=8)\n",
    "    #Extraire les fonctionnalités DAISY et les fonctions HOG de l'image et enregistrer dans une map\n",
    "        training_data_feature_map[fname]=[daisy_features,hog_feature]\n",
    "        daisy_descriptor_list=daisy_descriptor_list+list(daisy_features)\n",
    "    print ('Total daisy descriptors:',len(daisy_descriptor_list))\n",
    "    print ('...Enregistrement du modèle...')\n",
    "    daisy_cluster_model=cluster_daisy_features(daisy_descriptor_list,600) \n",
    "    daisy_cluster_model.n_clusters\n",
    "    joblib.dump(daisy_cluster_model, filename1)\n",
    "\n",
    "#Fonction pour extraire la fonction hybride des images en regroupant l'histogramme de DAISY et le descripteur de HOG après l'individu\n",
    "def extract_hog_feature_from_image(fname,daisy_cluster_model):\n",
    "    #Dans le cas où nous aurions rencontré le fichier lors de l'entrainement, les caractéristiques DAISY et HOG auraient déjà été calculées\n",
    "    if fname in training_data_feature_map:\n",
    "        daisy_features=training_data_feature_map[fname][0]\n",
    "        hog_feature=training_data_feature_map[fname][1]\n",
    "    else:\n",
    "        daisy_features,hog_feature=extract_daisy_and_hog_features_from_image(fname,daisy_step_size=8,daisy_radius=8)\n",
    "        \n",
    "    # Indiquer à quoi appartiennent les cluster de chaque caractéristique DAISY\n",
    "    img_clusters=daisy_cluster_model.predict(daisy_features) \n",
    "    cluster_freq_counts=pd.DataFrame(img_clusters,columns=['cnt'])['cnt'].value_counts()\n",
    "    bovw_vector=np.zeros(daisy_cluster_model.n_clusters) ##feature vector of size as the total number of clusters\n",
    "    for key in cluster_freq_counts.keys():\n",
    "        bovw_vector[key]=cluster_freq_counts[key]\n",
    "\n",
    "    #bovw_feature=bovw_vector/np.linalg.norm(bovw_vector)\n",
    "    hog_feature=hog_feature/np.linalg.norm(hog_feature)\n",
    "    return list(hog_feature)\n",
    "\n",
    "def plot_file(fname):\n",
    "    img_data=plt.imread(fname)\n",
    "    plt.imshow(rgb2gray(img_data),cmap='Greys_r')\n",
    "#Extraction de caractéristiques de données d'entrainement\n",
    "XTRAIN=[]\n",
    "YTRAIN=[]\n",
    "filename = 'trainedsvm.pkl'\n",
    "\n",
    "if os.path.exists(filename):\n",
    "   # load the model from disk\n",
    "    print ('...Chargement du fichier apprentissage...')\n",
    "    classifier = joblib.load(filename)\n",
    "else:\n",
    "    for i in tqdm(range(len(train_filenames))):\n",
    "        XTRAIN.append(extract_hog_feature_from_image(train_filenames[i],daisy_cluster_model))\n",
    "        YTRAIN.append(train_labels[i])\n",
    "    classifier=svm.SVC(C=10**1.6794140624999994, gamma=10**-0.1630955304365928, decision_function_shape='ovo') #paramètres de la fonction noyau SVM\n",
    "    print ('...Enregistrement du modèle...')\n",
    "    classifier.fit(XTRAIN,YTRAIN)\n",
    "    print ('...Enregistrement du fichier apprentissage...')\n",
    "    joblib.dump(classifier, filename)\n",
    "\n",
    "# Montrer un example de classification\n",
    "plot_file(test_filenames[4])\n",
    "print ('true label:',test_labels[4])\n",
    "feature_vector=extract_hog_feature_from_image(test_filenames[4],daisy_cluster_model)\n",
    "print ('prediction:',classifier.predict([feature_vector]))\n",
    "\n",
    "## Test de l'extraction des fonctions de données, ne faites que pour quelques instants si un test rapide est nécessaire\n",
    "XTEST=[]\n",
    "YTEST=[]\n",
    "for i in tqdm(range(len(test_filenames))):\n",
    "    XTEST.append(extract_hog_feature_from_image(test_filenames[i],daisy_cluster_model))\n",
    "    YTEST.append(test_labels[i])\n",
    "print ('Métriques du classifieur hybride')\n",
    "print ('Nombre instances test:',len(XTEST),len(YTEST))\n",
    "#Rapport de précision\n",
    "pred=classifier.predict(XTEST)\n",
    "       \n",
    "print ('Précision globale: \\n',accuracy_score(YTEST,pred))\n",
    "print (classification_report(YTEST, pred, target_names=category_names))\n",
    "print ('Matrice de confusion:\\n')\n",
    "pd.DataFrame(confusion_matrix(YTEST,pred),\n",
    "            columns=['MITinsidecity', 'bedroom', 'PARoffice', 'MITmountain', 'MITtallbuilding', 'MIThighway', 'MITcoast', 'livingroom', 'MITopencountry', 'MITstreet', 'MITforest', 'kitchen', 'CALsuburb'],\n",
    "               index=['MITinsidecity', 'bedroom', 'PARoffice', 'MITmountain', 'MITtallbuilding', 'MIThighway', 'MITcoast', 'livingroom', 'MITopencountry', 'MITstreet', 'MITforest', 'kitchen', 'CALsuburb']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
